---
title: "Estimation de la tendance-cycle avec des méthodes robustes aux points atypiques"
format: 
  html:
    fig-width: 9
    toc-depth: 3
    include-before-body: template/preambule.html
    output-file: index
  pdf:
    include-in-header: template/preambule.tex
    keep-tex: true
    classoption: french
    cite-method: biblatex
    pdf-engine: pdflatex
    papersize: A4
    fig-width: 7.5
    toc: false # to include abstract first
    fig-pos: 'H'
execute: 
  cache: true
lang: fr
echo: false
toc: true
tbl-cap-location: top
fig-cap-location: top
bibliography: biblio.bib
# csl: chicago-author-date.csl
---

```{r}
#| include: false
library(rjd3filters)
library(ggplot2)
library(patchwork)
source("../0-functions.R", local = knitr::knit_global())
source("../0-functions-plot.R", local = knitr::knit_global())
lc_f <- lp_filter()
robust_ff <- readRDS("../data/robust_ff.rds")
y_as_plot <- FALSE
add_y <- FALSE
apply_consistent_y_lims <- function(this_plot){
    num_plots <- length(this_plot$layers)
    y_lims <- lapply(1:num_plots, function(x) ggplot_build(this_plot[[x]])$layout$panel_scales_y[[1]]$range$range)
    min_y <- min(unlist(y_lims))
    max_y <- max(unlist(y_lims))
    this_plot & coord_cartesian(ylim = c(min_y, max_y))
}
```


# Introduction

L'analyse du cycle économique, et en particulier la détection rapide des points de retournement d'une série, est un sujet de première importance dans l'analyse de la conjoncture économique. 
Pour cela, les indicateurs économiques sont généralement corrigés des variations saisonnières.
Toutefois, afin d'améliorer leur lisibilité, il peut être nécessaire d'effectuer un lissage supplémentaire afin de réduire le bruit, et ainsi analyser la composante tendance-cycle.
Par construction, les méthodes d'extraction de tendance-cycle sont étroitement liées aux méthodes de désaisonnalisation.
En effet, afin d'estimer la composante saisonnière, les algorithmes de désaisonnalisation estiment préalablement une composante tendance-cycle.
Ainsi, même si les méthodes d'extraction de tendance-cycle sont généralement appliquées sur des séries corrigées des variations saisonnières, l'estimation de ces séries dépend également également des méthodes d'estimation de la tendance-cycle.

Les moyennes mobiles, ou les filtres linéaires, sont omniprésents dans les méthodes d'extraction du cycle économique et d'ajustement saisonnier^[
Une moyenne mobile est une méthode statistique qui consiste à appliquer une moyenne pondérée glissante à une série temporelle : à chaque date $t$ on calcule une moyenne pondérée de $p$ points passés et $q$ points futurs où $p,q\geq0$ dépend de la moyenne mobile.
]. 
Ainsi, la méthode de désaisonnalisation X-13ARIMA-SEATS utilise des moyennes mobiles de Henderson et des moyennes mobiles composites pour estimer les principales composantes d'une série chronologique. 
Au centre de la série, des filtres symétriques sont appliqués. 
Pour l'extraction de la tendance-cycle, le filtre symétrique le plus connu est celui de @henderson1916note, notamment utilisé dans l'algorithme de désaisonnalisation X-13ARIMA. 

Toutefois, ces moyennes mobiles, comme tout opérateur linéaire, sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.

D'autre part, les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.

<!-- Par ailleurs, comme notamment montré par @dagum1996new, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsqu'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 % des cycles de 9 ou 10 mois (généralement associés à du bruit plutôt qu'à la tendance-cycle). -->
<!-- Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois. -->
<!-- Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement. -->
<!-- Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier).  -->
<!-- C'est ainsi que le *Nonlinear Dagum Filter* (NLDF) a été développé et consiste à : -->

<!-- a. appliquer l'algorithme de correction des points atypiques de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ; -->

<!-- b. effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict et appliquer ensuite le filtre symétrique de 13 termes.  -->
<!-- En supposant une distribution normale cela revient à modifier 48 % des valeurs de l'irrégulier.    -->

<!-- Les *cascade linear filter* (CLF), notamment étudiés dans @dagumBianconcini2023, correspondent à une approximation des NLDF en utilisant un filtre de 13 termes et lorsque les prévisions sont obtenus à partir d'un modèle ARIMA(0,1,1) où $\theta=0,40.$ -->

<!-- Une piste d'étude serait alors d'étudier plus précisément l'effet des points atypiques sur l'estimation de la tendance-cycle et la détection des points de retournement, mais aussi d'explorer de nouveaux types de filtres asymétriques fondés sur des méthodes robustes (comme les régressions locales robustes, les médianes mobiles, etc.).  -->

<!-- En revanche, pour les estimations en temps réel, en raison de l'absence d'observations futures, toutes ces méthodes doivent s'appuyer sur des filtres asymétriques pour estimer les points les plus récents.  -->
<!-- Par exemple, même si X-13ARIMA-SEATS applique des moyennes symétriques aux prévisions obtenues à partir d'un modèle ARIMA, cela revient à appliquer des filtres asymétriques en fin de série, car les valeurs prédites sont des combinaisons linéaires de valeurs passées.  -->

<!-- Si ces moyennes mobiles asymétriques ont de bonnes propriétés concernant la taille des révisions futures induites par le processus de lissage^[Voir par exemple @pierce1980SA.], elles induisent également, par construction, des déphasages qui retardent en général la détection en temps réel des points de retournement. -->


# Méthodes utilisées

## Méthodes robustes


$$
y_{t+i}=\mu_t+\beta_ti+\varepsilon_{t,i}\text{ et }
r_{t+i}=y_{t+i}-\hat\mu_t-\hat\beta_ti
$$

- Médiane mobile :

$$
\hat\mu_t=\underset{i=-m,\dots,m}{\med}y_{t+i}
$$

- Least Median of Squares regression (LMS)
$$
(\hat\mu_t,\beta_t)=\underset{\hat\mu_t,\beta_t}{\argmin}\left\{ \underset{i=-m,\dots,m}{\med}r^2_{t+i} \right\}
$$

- Least Trimmed Squares regression (LTS)
$$
(\hat\mu_t,\hat\beta_t)=\underset{\hat\mu_t,\beta_t}{\argmin}\left\{ \sum_{i=-m}^m r^2_{t+i} \right\}
$$

- Repeated Median regression (RM)
$$
\hat{\beta}_{t}=\underset{i=1,\dots,n}{\med}\left\{ \underset{i\ne j}{\med}\frac{y_{t+i}-y_{t+j}}{i-j} \right\}
$$
et 
$$
\hat\mu_t=\underset{i=1,\dots,n}{\med}\left\{ y_{t+i}-i\hat{\beta}_{t} \right\}
$$
- Least Quartile Difference regression (LQD)

- Deepest Regression (DR)

## Moyennes mobiles linéaires

Les moyennes mobiles classiques peuvent être obtenues par analogie avec la régression polynomiale locale.
En reprenant les notations de @proietti2008, on suppose que notre série temporelle $y_t$ peut être décomposée en :
$$
y_t=TC_t+\varepsilon_t,
$$
où $TC_t$ est la tendance-cycle et $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ est le bruit. 
La tendance-cycle $\TC_t$ est localement approchée par un polynôme de degré $d$, de sorte que dans un voisinage $h$ de $t$ $TC_t\simeq m_{t}$ avec :
$$
\forall j\in\left\{-h,-h+1,\dots,h\right\},\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}.
$$
En notation matricielle :
$$
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{\boldsymbol y}=\underbrace{\begin{pmatrix}1 & -h & h^{2} & \cdots & (-h)^{d}\\
1 & -(h-1) & (h-1)^{2} & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & h-1 & (h-1)^{2} & \cdots & (h-1)^{d}\\
1 & h & h^{2} & \cdots & h^{d}
\end{pmatrix}}_{\boldsymbol X}\underbrace{\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d}
\end{pmatrix}}_{\boldsymbol \beta}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\boldsymbol \varepsilon}.
$$ {#eq-lpp-mat}

L'estimation des paramètres $\boldsymbol \beta$ peut être obtenue moindres carrés pondérés --- *weighted least squares* (WLS) --- à partir d'un ensemble de poids $(\kappa_j)_{-h\leq j \leq h}$ appelés noyaux.
En notant $\boldsymbol K=diag(\kappa_{-h},\dots,\kappa_{h})$ il vient $\hat{\boldsymbol\beta}=(\transp{\boldsymbol X}\boldsymbol K\boldsymbol X)^{-1}\transp{\boldsymbol X}\boldsymbol K\boldsymbol y.$
Avec $\boldsymbol e_1=\transp{\begin{pmatrix}1 &0 &\cdots&0 \end{pmatrix}}$, l'estimation de la tendance-cycle est :
$$
\widehat{TC}_t=\boldsymbol e_{1}\hat{\boldsymbol \beta}=\transp{\boldsymbol \theta}\boldsymbol y=\sum_{j=-h}^{h}\theta_{j}y_{t-j}\text{ avec }\boldsymbol \theta=\boldsymbol K\boldsymbol X(\transp{\boldsymbol X}\boldsymbol K\boldsymbol X)^{-1}\boldsymbol e_{1}.
$$ {#eq-mmsym}
En somme, l'estimation de la tendance $\hat{m}_{t}$ est obtenue en appliquant une moyenne mobile symétrique $\boldsymbol \theta$ à $y_t$.

On retrouve la moyenne mobile de Henderson en utilisant les noyaux :
$$
\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right].
$$
De plus, puisque $\transp{\boldsymbol X}\boldsymbol \theta=\boldsymbol e_{1}$ on a:
$$
\sum_{j=-h}^{h}\theta_{j}=1,\quad\forall r\in\left\{1,2,\dots,d\right\}:\sum_{j=-h}^{h}j^{r}\theta_{j}=0.
$$
La moyenne mobile $\boldsymbol \theta$ conserve donc les tendances polynomiales de degré $d$.


En ajoutant un régresseur $O_t$ on peut donc obtenir une nouvelle moyenne mobile donc l'estimation de $\beta_0$ prendrait en compte l'effet modélisé par $x_t$
$$
y_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i} + \gamma O_{t+j}+\varepsilon_{t+j}
$$

Deux cas sont ici étudiés :

- Points atypiques additifs (AO) : un choc soudain à une certaine date puis un retour à la normal.
Si le choc est à la date $t_0$ le régresseur utilisé est alors $x_t=1_{t=t_0}$.

- Rupture en niveau (LS) : un changement de niveau à une certaine date.
Si le choc est à la date $t_0$ le régresseur utilisé est alors $x_t=-1_{t< t_0}$ pour l'estimation de $TC_t$ avec $t\leq t_0$ et $x_t=1_{t\geq t_0}$ sinon.

Pour le cas asymétrique, @proietti2008 proposent une méthode générale pour construire les filtres asymétriques qui permet de faire un compromis biais-variance.
Il s'agit d'une généralisation des filtres asymétriques de @musgrave1964set (utilisés dans l'algorithme de désaisonnalisation X-13ARIMA).

On modélise ici la série initiale par :
$$
\boldsymbol y=\boldsymbol U\boldsymbol \gamma+\boldsymbol Z\boldsymbol \delta+\boldsymbol \varepsilon,\quad
\boldsymbol \varepsilon\sim\mathcal{N}(\boldsymbol 0,\boldsymbol D).
$${#eq-lpgeneralmodel}
où $[\boldsymbol U,\boldsymbol Z]$ est de rang plein et forme un sous-ensemble des colonnes de $\boldsymbol X$.
L'objectif est de trouver un filtre $\boldsymbol v$ qui minimise l'erreur quadratique moyenne de révision (au filtre symétrique $\boldsymbol \theta$) sous certaines contraintes.
Ces contraintes sont représentées par la matrice $\boldsymbol U=\begin{pmatrix}\boldsymbol U_{p}'&\boldsymbol U_{f}'\end{pmatrix}'$ : $\boldsymbol U_p'\boldsymbol v=\boldsymbol U'\boldsymbol \theta$ (avec $\boldsymbol U_p$ la matrice $(h+q+1)\times (d+1)$ qui contient les observations de la matrice $\boldsymbol U$ connues lors de l'estimation par le filtre asymétrique).
C'est ce qui est implémenté dans la fonction `rjd3filters::mmsre_filter()`.

Lorsque $\boldsymbol U=\boldsymbol X$, la contrainte équivaut à préserver les polynômes de degré $d$ : on retrouve les filtres directs symétriques avec $\boldsymbol D=\boldsymbol K^{-1}$.

Lorsque $\boldsymbol U$ correspond aux $d^*+1$ premières colonnes de $\boldsymbol X$, $d^*<d$, la contrainte consiste à reproduire des tendances polynomiales de degré $d^*$.
Cela introduit du biais mais réduit la variance. 
Le filtre de Musgrave se retrouve en modélisant $y_t$ linéaire ($d=1$) et $v$ préserve les constantes ($d^*=0$) et en prenant le filtre d'Henderson comme filtre symétrique.
Les filtres asymétriques robustes sont construits en ajoutant un régresseur supplémentaire à $\boldsymbol U$ (estimation sans biais de l'outlier) et en utilisant comme filtre symétrique celui construit en utilisant le même régresseur.

## Construction d'intervalles de confiance pour des moyennes mobiles

Soit $y_1,\dots,y_n$ une série chronologique observée.
On suppose qu'elle peut être décomposée en 
$$
y_t=\mu_t+\varepsilon_t,
$$
où $\mu_t$ est une composante observée déterministe et $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ est le bruit.

Soit $\theta = \begin{pmatrix}\theta_{-p},\dots,\theta_f\end{pmatrix}$ une moyenne mobile permettant d'estimer une composante inobservable $\mu_t$ (dans notre cas la tendance-cycle $TC_t$) à partir de $y_t$.
Cette estimation est donnée par $\hat \mu_t = \sum_{i=-p}^{+f}\theta_iy_{t+i}.$

Un intervalle de confiance de $\mathbb E[\hat{\mu}_t]$ peut être calculé à partir de la formule :
$$
I_t=\left[\hat{\mu}_t - q_{\alpha}\sqrt{\hat \sigma^2}\sqrt{\sum_{i=-p}^{+f}\theta_i^2};
 \hat{\mu}_t  + q_{\alpha}\sqrt{\hat \sigma^2}\sqrt{\sum_{i=-p}^{+f}\theta_i^2}\right]
$$
où 
$$
\hat\sigma^2=\frac{1}{n-p-f}\sum_{t=p+1}^{n-f}\frac{(y_t-\widehat{\mu}_t)^2}{1-2\theta_0^2+\sum_{i=-p}^{+f} \theta_i^2}.
$$
et $q_\alpha$ est le quantile d'ordre $\alpha$ d'une certaine loi de Student.
C'est un intervalle de confiance de $\mu_t$ lorsque l'on a un estimateur sans biais de $\mu_t$ ($\mathbb E[\hat{\mu}_t]=\sum_{i=-p}^{+f}\theta_iy_{t+i}=\mu_t$), ce qui n'est généralement pas le cas mais il est négligeablelorsque la fenêtre $p+f+1$ est petite.

Ces formules ces retrouvent par analogie avec la régression polynomiale locale.
En reprenant les notations de @Loader1999 pour la régression polynomiale locale et en adaptant à l'utilisation de moyennes mobiles, la variance $\hat\sigma^2$ peut être estimée par la somme des carrés des résidus normalisés :
$$
\hat\sigma^2=\frac{1}{(n-p-f)-2\nu_1+\nu_2}\sum_{t=p+1}^{n-f}(y_t-\widehat{\mu}_t)^2.
$$
$n-p-f$ termes sont utilisés car avec la moyenne mobile $\theta$ seulement $n-p-f$ observations peuvent être utilisées pour estimer $\sigma^2$.
$\nu_1$ et $\nu_2$ sont deux définitions de degrés de liberté d'une estimation locale (généralisation du nombre de paramètres d'un modèle paramétrique).
Notons $\boldsymbol H$ la *matrice chapeau* de taille $n\times n$ permettant de faire correspondre les données aux valeurs estimées :
$$
\begin{pmatrix}
\hat{\mu}_1\\ \vdots \\ \hat{\mu}_n
\end{pmatrix} = \boldsymbol H \boldsymbol Y
\text{ avec }
\boldsymbol Y = \begin{pmatrix}
y_1\\ \vdots \\ y_n
\end{pmatrix}.
$$
En considérant par convention que $\hat\mu_1=\dots=\hat{\mu}_p=\hat\mu_{n-f+1}=\dots=\hat\mu_n=0$ (puisque l'on ne peut pas estimer ces quantités avec la moyenne mobile $\boldsymbol \theta$), on a donc :
$$
\boldsymbol H=\begin{pmatrix}
&&\boldsymbol0_{p\times n} \\
\theta_{-p} & \cdots & \theta_f  & 0 & \cdots\\
0 & \theta_{-p} & \cdots & \theta_f  & 0 & \cdots
\\ & \ddots &&&\ddots\\
0 &\cdots&0& \theta_{-p} & \cdots & \theta_f \\
&&\boldsymbol0_{f\times n} 
\end{pmatrix},
$$
où $\boldsymbol 0_{p\times n}$ est la matrice de taille $p\times n$ ne contenant que de zéros.
On a :
$$
\begin{cases}
\nu_1 =\tr (L) = (n-p-f)\theta_0\\
\nu_2 = \tr (\transp{L}L) = (n-p-f) \sum_{i=-p}^{+f} \theta_i^2
\end{cases}.
$$
Si les bruits $\varepsilon_t$ sont indépendants et de variance $\sigma^2$, alors :
$$
\V{Y_t-\hat_{\mu}_t}=\sigma^2 - 
2\underbrace{\cov{Y_t,\hat{\mu}_t}}_{=\theta_0\sigma^2} + 
\underbrace{\V{\hat{\mu}_t}}_{=\sigma^2\sum_{i=-p}^{+f} \theta_i^2}
$$
et l'on a donc :
$$
\E{\hat\sigma^2}=\sigma^2 + \frac{1}{(n-p-f)-2\nu_1+\nu_2}\sum_{t=p+1}^{n-f}(\E{\hat \mu_t}-\mu_t)^2.
$$
L'estimateur $\hat\sigma^2$ est donc sans biais si $\hat\mu_t$ l'est aussi.


La somme des carrés des résidus peut s'écrire sous forme quadratique :
$$
\sum_{t=p+1}^{n-f}(y_t-\widehat{\mu}_t)^2 =
\transp{\boldsymbol Y}\boldsymbol \Gamma\boldsymbol Y 
$$
où :
$$
\boldsymbol \Gamma = \transp{(\boldsymbol I - \boldsymbol H)}(\boldsymbol I - \boldsymbol H\boldsymbol)\text{ avec }
\boldsymbol I = \begin{pmatrix}
&\boldsymbol0_{p\times n} \\
\boldsymbol 0_{(n-p-f)\times p} &I_{n-p-f} & \boldsymbol 0_{(n-p-f)\times f}  \\
&\boldsymbol0_{f\times n} 
\end{pmatrix}
$$
et $I_{n-p-f}$ la matrice identité de taille $n-p-f.$
On a donc :
$$
\hat\sigma^2=\frac{1}{\tr{\boldsymbol \Gamma}}\transp{\boldsymbol Y}\boldsymbol \Gamma\boldsymbol Y
$$
et 
$$
\begin{cases}
\E{\hat\sigma^2}=\sigma^2\tr(\boldsymbol \Gamma)\\
\V{\hat\sigma^2}=2\sigma^4\frac{\tr(\boldsymbol \Gamma^2)}{\tr(\boldsymbol \Gamma)^2}
\end{cases}.
$$
En notant $\nu = \tr(\boldsymbol \Gamma)^2 / \tr(\boldsymbol \Gamma^2)$, on a donc :
$$
\begin{cases}
\E{\nu\frac{\hat\sigma^2}{\sigma^2}}=\nu\\
\V{\nu\frac{\hat\sigma^2}{\sigma^2}}=2\nu
\end{cases}.
$$
Les deux premiers moments de $\nu \hat\sigma^2/\sigma^2$ sont donc identiques à ceux d'un loi du chi-deux avec $\nu$ degrés de liberté : on peut donc approximer la distribution de $\hat\sigma^2$ par une loi du chi-deux.

Puisque $\V{\mu_t}=\sigma^2\sum_{i=-p}^{+f} \theta_i^2$, on a donc :
$$
\frac{\hat\mu_t-\E{\mu_t}}{\sqrt{\V{\mu_t}}}\sim\mathcal{T}(\nu)
$$
Le paramètre $\nu$ peut être calculé analytiquement en reconstruisant la matrice $\boldsymbol \Gamma$ (c'est ce qui est fait par défaut dans `rjd3filters::confint_filter()`).
Il peut aussi être approximé par $\tr(\boldsymbol \Gamma)$ (`rjd3filters::confint_filter(exact_df = FALSE)`) ce qui permet de réduire le temps de calcul^[
Sur une série mensuelle de 19 ans, le temps de calcul est d'environ 0,1 secondes et est divisé d'en moyenne 4 000 en utilisant l'approximation.
].

# Résultats

## Additive outlier (A0)

### IPI voitures

#### Août 1998


```{r}
res <- readRDS(file.path("..", "results", "AO", "ipi_voitures98.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est <- lapply(graph_est,function(p){
	p$CLF <- NULL
	p
})
graph_y <- plot_y(res)
graph_ci <- lapply(
	res$out, plot_confint,
	data = res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```


```{r}
#| label: fig-ipi_voitures98-out1-y
#| fig.cap: "Série brute"
graph_y[[1]]
```

```{r}
#| label: fig-ipi_voitures98-out1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est[[1]], ncol = 3) 
```


```{r}
#| label: fig-ipi_voitures98-out1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci[[1]], ncol = 3)
```

#### Août 1999


```{r}
#| label: fig-ipi_voitures98-out2-y
#| fig.cap: "Série brute"
graph_y[[2]]
```

```{r}
#| label: fig-ipi_voitures98-out2-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est[[2]], ncol = 3)
```


```{r}
#| label: fig-ipi_voitures98-out2-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci[[2]], ncol = 3)
```


#### 2004


```{r}
res <- readRDS(file.path("..", "results", "AO", "ipi_voitures2004.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```

```{r}
#| label: fig-ipi_voitures2004-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-ipi_voitures2004-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ipi_voitures2004-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

## Level shift (LS)

### IPI petrole brut

```{r}
res <- readRDS(file.path("..", "results", "LS", "ipi_petrole_brut10.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ls"]])
```

```{r}
#| label: fig-ipi_petrole_brut10-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-ipi_petrole_brut10-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ipi_petrole_brut10-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```



## Deux LS

### Retailx

```{r}
res <- readRDS(file.path("..", "results", "LSLS", "retailx2008.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["lsls"]])
```

```{r}
#| label: fig-retailx2008-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-retailx2008-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-retailx2008-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```


## AO puis LS

### Immatriculation de véhicules neufs

```{r}
res <- readRDS(file.path("..", "results", "AOLS", "immat2018.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["aols"]])
```

```{r}
#| label: fig-imat2018-y
#| fig.cap: "Série brute"
graph_y
```



```{r}
#| label: fig-imat2018-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-imat2018-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

## Point de retournement

### Retailx

```{r}
res <- readRDS(file.path("..", "results", "AO", "retailx2007.rds"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```

```{r}
#| label: fig-retailx2007-y
#| fig.cap: "Série brute"
graph_y
```



```{r}
#| label: fig-retailx2007-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-retailx2007-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

### ce160v

```{r}
res <- readRDS(file.path("..", "results", "AO", "ce160v2001.rds"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```

```{r}
#| label: fig-ce160v2001-y
#| fig.cap: "Série brute"
graph_y
```



```{r}
#| label: fig-ce160v2001-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ce160v2001-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

# Annexe



## Moyennes mobiles utilisées


```{r}
#| label: fig-mm-musgrave
#| fig.cap: "Henderson et filtre de musgrave"
ggdemetra3::ggplot_coef(lp_filter(), q= 0:6)
```

```{r}
#| label: fig-clf
#| fig.cap: "Cascade linear filter"
ggdemetra3::ggplot_coef(CLF, q= 0:6)
```

```{r}
#| label: fig-clfnn
#| fig.cap: "Cascade linear filter et cut and normalize"
ggdemetra3::ggplot_coef(CLF_CN, q= 0:6)
```

```{r}
#| label: fig-robust-ao
#| fig.cap: "Filtre robuste AO présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$ao$t0, q= 0:7)
```

```{r}
#| label: fig-robust-aols
#| fig.cap: "Filtre robuste AO puis LS présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$aols$`t-1`, q= 0:7)
```

```{r}
#| label: fig-robust-ls
#| fig.cap: "Filtre robuste LS présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$ls$t0, q= 0:7)
```

```{r}
#| label: fig-robust-lsls
#| fig.cap: "Filtre robuste deux LS présents aux deux dernières dates"
ggdemetra3::ggplot_coef(robust_ff$lsls$`t-1`, q= 0:7)
```
