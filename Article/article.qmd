---
title: "Estimation de la tendance-cycle avec des méthodes robustes aux points atypiques"
format: 
  html:
    fig-width: 9
    toc-depth: 3
    include-before-body: template/preambule.html
    output-file: index
  pdf:
    include-in-header: template/preambule.tex
    keep-tex: true
    classoption: french
    cite-method: biblatex
    pdf-engine: pdflatex
    papersize: A4
    fig-width: 7.5
    toc: false # to include abstract first
    fig-pos: 'H'
execute: 
  cache: true
lang: fr
echo: false
toc: true
number-sections: true
tbl-cap-location: top
fig-cap-location: top
bibliography: biblio.bib
# csl: chicago-author-date.csl
---

```{r}
#| include: false
library(rjd3filters)
library(ggplot2)
library(patchwork)
source("../0-functions.R", local = knitr::knit_global())
source("../0-functions-plot.R", local = knitr::knit_global())
lc_f <- lp_filter()
robust_ff <- readRDS("../data/robust_ff.rds")
y_as_plot <- FALSE
add_y <- FALSE
apply_consistent_y_lims <- function(this_plot){
    num_plots <- length(this_plot$layers)
    y_lims <- lapply(1:num_plots, function(x) ggplot_build(this_plot[[x]])$layout$panel_scales_y[[1]]$range$range)
    min_y <- min(unlist(y_lims))
    max_y <- max(unlist(y_lims))
    this_plot & coord_cartesian(ylim = c(min_y, max_y))
}
```


# Introduction

L'analyse du cycle économique, et en particulier la détection rapide des points de retournement d'une série, est un sujet de première importance dans l'analyse de la conjoncture économique. 
Pour cela, les indicateurs économiques sont généralement corrigés des variations saisonnières.
Toutefois, afin d'améliorer leur lisibilité, il peut être nécessaire d'effectuer un lissage supplémentaire afin de réduire le bruit, et ainsi analyser la composante tendance-cycle.
Par construction, les méthodes d'extraction de tendance-cycle sont étroitement liées aux méthodes de désaisonnalisation.
En effet, afin d'estimer la composante saisonnière, les algorithmes de désaisonnalisation estiment préalablement une composante tendance-cycle.
Ainsi, même si les méthodes d'extraction de tendance-cycle sont généralement appliquées sur des séries corrigées des variations saisonnières, l'estimation de ces séries dépend également également des méthodes d'estimation de la tendance-cycle.

Les moyennes mobiles, ou les filtres linéaires, sont omniprésents dans les méthodes d'extraction du cycle économique et d'ajustement saisonnier^[
Une moyenne mobile est une méthode statistique qui consiste à appliquer une moyenne pondérée glissante à une série temporelle : à chaque date $t$ on calcule une moyenne pondérée de $p$ points passés et $q$ points futurs où $p,q\geq0$ dépend de la moyenne mobile.
]. 
Ainsi, la méthode de désaisonnalisation X-13ARIMA-SEATS utilise des moyennes mobiles de Henderson et des moyennes mobiles composites pour estimer les principales composantes d'une série chronologique. 
Au centre de la série, des filtres symétriques sont appliqués. 
Pour l'extraction de la tendance-cycle, le filtre symétrique le plus connu est celui de @henderson1916note, notamment utilisé dans l'algorithme de désaisonnalisation X-13ARIMA. 

Toutefois, ces moyennes mobiles, comme tout opérateur linéaire, sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.

D'autre part, les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.

<!-- Par ailleurs, comme notamment montré par @dagum1996new, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsqu'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 % des cycles de 9 ou 10 mois (généralement associés à du bruit plutôt qu'à la tendance-cycle). -->
<!-- Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois. -->
<!-- Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement. -->
<!-- Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier).  -->
<!-- C'est ainsi que le *Nonlinear Dagum Filter* (NLDF) a été développé et consiste à : -->

<!-- a. appliquer l'algorithme de correction des points atypiques de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ; -->

<!-- b. effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict et appliquer ensuite le filtre symétrique de 13 termes.  -->
<!-- En supposant une distribution normale cela revient à modifier 48 % des valeurs de l'irrégulier.    -->

<!-- Les *cascade linear filter* (CLF), notamment étudiés dans @dagumBianconcini2023, correspondent à une approximation des NLDF en utilisant un filtre de 13 termes et lorsque les prévisions sont obtenus à partir d'un modèle ARIMA(0,1,1) où $\theta=0,40.$ -->

<!-- Une piste d'étude serait alors d'étudier plus précisément l'effet des points atypiques sur l'estimation de la tendance-cycle et la détection des points de retournement, mais aussi d'explorer de nouveaux types de filtres asymétriques fondés sur des méthodes robustes (comme les régressions locales robustes, les médianes mobiles, etc.).  -->

<!-- En revanche, pour les estimations en temps réel, en raison de l'absence d'observations futures, toutes ces méthodes doivent s'appuyer sur des filtres asymétriques pour estimer les points les plus récents.  -->
<!-- Par exemple, même si X-13ARIMA-SEATS applique des moyennes symétriques aux prévisions obtenues à partir d'un modèle ARIMA, cela revient à appliquer des filtres asymétriques en fin de série, car les valeurs prédites sont des combinaisons linéaires de valeurs passées.  -->

<!-- Si ces moyennes mobiles asymétriques ont de bonnes propriétés concernant la taille des révisions futures induites par le processus de lissage^[Voir par exemple @pierce1980SA.], elles induisent également, par construction, des déphasages qui retardent en général la détection en temps réel des points de retournement. -->


# Méthodologie

L'hypothèse de base utilisée dans les méthodes de décomposition des séries temporelles est que la série temporelle observée, $y_t$, peut être décomposée en une composante de signal $\mu_t$ et une composante erratique $\varepsilon_t$ (appelée composante irrégulière) :
$$
f(y_t)=\mu_t+\varepsilon_t
$$
où $f$ désigne une transformation appropriée (généralement logarithmique ou aucune transformation).
Pour simplifier les notations ultérieures, $y_t$ désignera la série observée transformée.
La composante de bruit $\varepsilon_t$ est généralement supposée être un bruit blanc.
En supposant que la série temporelle initiale est désaisonnalisée (ou sans saisonnalité), le signal $\mu_t$ représente la tendance (variations sur une longue période) et cycle (mouvements cycliques superposés à la tendance à long terme), estimés ici conjointement et appelé tendance-cycle $TC_t$.
Autour d'un voisinage $h$ de $t$, cette composante peut être approximée localement par un polynôme de degré $d$ :
$$
TC_{t+i} = \sum_{j=0}^d\beta_j{i}^j+\xi_{t+i}\quad\forall i\in\{-h,-h+1,\dots,t+h\}
$$
avec $\xi_t$ un processus stochastique non corrélé avec $\varepsilon_t$.
Même si certains articles modélisent $\xi_t$ et $\varepsilon_t$ séparément [voir par exemple @GrayThomson2002], une hypothèse habituelle, utilisée dans cet article, est de rassembler $\xi_t$ et $\varepsilon_t$^[
Cela revient à supposer que le biais d'approximation de la tendance-cycle par un polynôme local est nul.
].
Ainsi, la tendance-cycle $TC_t$ est considérée comme déterministe et modélisée comme une tendance polynomiale de degré $d$.
Les coefficients $(\beta_0,\dots,\beta_d)$ peuvent être estimés par la méthode des moindres carrés pondérés.
L'estimation $\hat \beta_0$ fournit l'estimation du cycle de tendance $\widehat{TC}_t$ et on peut montrer que cela équivaut à appliquer une moyenne mobile.

L'estimation de $\widehat{TC}_t$ étant faite par un méthode linéaire, elle est sensible à la présence de points atypiques.
L'objectif de cet article est d'étudier différentes méthodes robustes à la présence de points atypiques ainsi que de proposer une méthode de constructions de moyennes mobiles robuste, et de comparer les estimations faites en temps réel par ces différentes méthodes.


## Méthodes robustes

Dans cette étude nous étudions six méthodes robustes d'estimation locale de la moyenne implémentées dans la fonction `robfilter::robreg.filter()` [@robfilter].
Dans ce package, la tendance-cycle est supposée être localement linéaire :
$$
y_{t+i}=\underbrace{\beta_{0,t}+\beta_{1,t}i}_{TC_{t+i}}+\varepsilon_{t,i}
$$
Et l'on note :
$$
r_{t+i}=y_{t+i}-(\beta_{0,t}+\beta_{1,t}i).
$$
On a donc :
$$
\widehat{TC}_t = \hat\beta_{0,t}.
$$

Les différentes méthodes étudiées sont :

- Médiane mobile :

$$
\widehat{TC}_t =
\underset{i=-h,\dots,h}{\med}y_{t+i}
$$

- Régression des moindres carrés médians --- *Least Median of Squares* (LMS) [@lms] :
$$
(\widehat{TC}_t,\hat\beta_{1,t})=
\underset{\beta_{0,t},\beta_{1,t}}{\argmin}
\left\{ \underset{i=-h,\dots,h}{\med}r^2_{t+i} \right\}
$$

- Régression des moindre carrés élagués --- *Least Trimmed Squares* (LTS) [@lts]
$$
(\widehat{TC}_t,\hat\beta_{1,t})=
\underset{\beta_{0,t},\beta_{1,t}}{\argmin}
\left\{ \sum_{i=-h}^h r^2_{t+i} \right\}
$$

- Régression médiane répétée --- *Repeated Median* (RM) [@rm]
$$
\hat{\beta}_{1,t}=
\underset{i=-h,\dots,h}{\med}
\left\{ \underset{i\ne j}{\med}\frac{y_{t+i}-y_{t+j}}{i-j} \right\}
$$
et 
$$
\widehat{TC}_t=
\underset{i=-h,\dots,h}{\med}
\left\{ y_{t+i}-i\hat{\beta}_{1,t} \right\}
$$
- Régression des moindres quartiles différenciés --- *Least Quartile Difference* (LQD) [@lqd].
En notant $r_{i,j} = r_i-r_j$
$$
\underset{\beta_{1,t}}{\argmin}
Q_{2h+1}(r_{t-h},\dots,r_{t+h})
$$
avec
$$
Q_{2h+1}(r_{t-h},\dots,r_{t+h}) = \left\{
|r_i-r_j|;i<j
\right\}_{\binom{h_p}{2}:\binom{2h+1}{2}}
$$
le quantile d'ordre $\binom{h_p}{2}$ sur les $\binom{2h+1}{2}$ éléments de l'ensemble $\{|r_i-r_j|;i<j\}$ et $h_p=[(2h+1+p+1)/2]$ avec $p=1$ le nombre de régresseurs (hors constante).
La fonction objectif ne dépendant pas de la constante, elle est estimée a posteriori par exemple en utilisant la formule :
$$
\widehat{TC}_t =
\underset{i=-h,\dots,h}{\med}y_{t+i}-\hat \beta_{1,t}i.
$$

- Régression profonde --- *Deepest Regression* (DR) [@DeepestRegression]
$$
(\widehat{TC}_t=\hat\beta_{0,t},\hat\beta_{1,t})=\underset{\tilde\beta_{0,t},\tilde\beta_{1,t}}{\argmax}\left\{ rdepth((\tilde\mu,\tilde\beta),\boldsymbol y) \right\}
$$
où la profondeur de la regression --- *regression depth* $rdepth$ --- d'un ajustement $(\tilde\mu,\tilde\beta)$ à un échantillon $\boldsymbol y$ est défini comme :
$$
rdepth((\tilde\mu,\tilde\beta),\boldsymbol y) = \underset{-h\leq i\leq h}{\min}
\left\{ \min \left\{ L^+(i) + R^-(i);R^+(i) + L^-(i) \right\}\right\}
$$
avec :
$$
\begin{cases}
L^+(i) = L^+_{\tilde\mu,\tilde\beta}(i) = cardinal \left\{ j\in\{-h,\dots,i\} : r_j(\tilde\mu,\tilde\beta)\leq 0 \right\} \\
R^-(i) = R^-_{\tilde\mu,\tilde\beta}(i) = cardinal \left\{ j\in\{i+1,\dots,h\} : r_j(\tilde\mu,\tilde\beta)< 0 \right\}
\end{cases}
$$
et $L^-(i)$ et $R^+(i)$ définis de manière analogue.

Pour l'estimaion en temps réel, le package `robfilter` utilise les mêmes algorithmes mais en n'utilisant que les données disponibles (donc en utilisant plus de points avant $t$ qu'après $t$).

## Moyennes mobiles linéaires

### Moyenne mobile de Henderson et de Musgrave
Les moyennes mobiles classiques peuvent être obtenues par analogie avec la régression polynomiale locale.
En reprenant les notations de @proietti2008, on suppose que notre série temporelle $y_t$ peut être décomposée en :
$$
y_t=TC_t+\varepsilon_t,
$$
où $TC_t$ est la tendance-cycle et $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ est le bruit. 
La tendance-cycle $TC_t$ est localement approchée par un polynôme de degré $d$, de sorte que dans un voisinage $h$ de $t$ $TC_t\simeq m_{t}$ avec :
$$
\forall j\in\left\{-h,-h+1,\dots,h\right\},\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}.
$$
En notation matricielle :
$$
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{\boldsymbol y}=\underbrace{\begin{pmatrix}1 & -h & h^{2} & \cdots & (-h)^{d}\\
1 & -(h-1) & (h-1)^{2} & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & h-1 & (h-1)^{2} & \cdots & (h-1)^{d}\\
1 & h & h^{2} & \cdots & h^{d}
\end{pmatrix}}_{\boldsymbol X}\underbrace{\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d}
\end{pmatrix}}_{\boldsymbol \beta}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\boldsymbol \varepsilon}.
$$ {#eq-lpp-mat}

L'estimation des paramètres $\boldsymbol \beta$ peut être obtenue moindres carrés pondérés --- *weighted least squares* (WLS) --- à partir d'un ensemble de poids $(\kappa_j)_{-h\leq j \leq h}$ appelés noyaux.
En notant $\boldsymbol K=diag(\kappa_{-h},\dots,\kappa_{h})$ il vient $\hat{\boldsymbol\beta}=(\transp{\boldsymbol X}\boldsymbol K\boldsymbol X)^{-1}\transp{\boldsymbol X}\boldsymbol K\boldsymbol y.$
Avec $\boldsymbol e_1=\transp{\begin{pmatrix}1 &0 &\cdots&0 \end{pmatrix}}$, l'estimation de la tendance-cycle est :
$$
\widehat{TC}_t=\boldsymbol e_{1}\hat{\boldsymbol \beta}=\transp{\boldsymbol \theta}\boldsymbol y=\sum_{j=-h}^{h}\theta_{j}y_{t-j}\text{ avec }\boldsymbol \theta=\boldsymbol K\boldsymbol X(\transp{\boldsymbol X}\boldsymbol K\boldsymbol X)^{-1}\boldsymbol e_{1}.
$$ {#eq-mmsym}
En somme, l'estimation de la tendance $\hat{m}_{t}$ est obtenue en appliquant une moyenne mobile symétrique $\boldsymbol \theta$ à $y_t$.

On retrouve la moyenne mobile de Henderson en utilisant les noyaux :
$$
\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right].
$$
<!-- De plus, puisque $\transp{\boldsymbol X}\boldsymbol \theta=\boldsymbol e_{1}$ on a: -->
<!-- $$ -->
<!-- \sum_{j=-h}^{h}\theta_{j}=1,\quad\forall r\in\left\{1,2,\dots,d\right\}:\sum_{j=-h}^{h}j^{r}\theta_{j}=0. -->
<!-- $$ -->
<!-- La moyenne mobile $\boldsymbol \theta$ conserve donc les tendances polynomiales de degré $d$. -->

Pour le cas asymétrique, @proietti2008 proposent une méthode générale pour construire les filtres asymétriques qui permet de faire un compromis biais-variance.
Il s'agit d'une généralisation des filtres asymétriques de @musgrave1964set (utilisés dans l'algorithme de désaisonnalisation X-13ARIMA).

On modélise ici la série initiale par :
$$
\boldsymbol y=\boldsymbol U\boldsymbol \gamma+\boldsymbol Z\boldsymbol \delta+\boldsymbol \varepsilon,\quad
\boldsymbol \varepsilon\sim\mathcal{N}(\boldsymbol 0,\boldsymbol D).
$${#eq-lpgeneralmodel}
où $[\boldsymbol U,\boldsymbol Z]$ est de rang plein et forme un sous-ensemble des colonnes de $\boldsymbol X$.
L'objectif est de trouver un filtre $\boldsymbol v$ qui minimise l'erreur quadratique moyenne de révision (au filtre symétrique $\boldsymbol \theta$) sous certaines contraintes.
Ces contraintes sont représentées par la matrice $\boldsymbol U=\transp{\begin{pmatrix}\transp{\boldsymbol U_{p}}&\transp{\boldsymbol U_{f}}\end{pmatrix}}$ : $\transp{\boldsymbol U_p}\boldsymbol v=\transp{\boldsymbol U}\boldsymbol \theta$ (avec $\boldsymbol U_p$ la matrice $(h+q+1)\times (d+1)$ qui contient les observations de la matrice $\boldsymbol U$ connues lors de l'estimation par le filtre asymétrique).
C'est ce qui est implémenté dans la fonction `rjd3filters::mmsre_filter()`.

Lorsque $\boldsymbol U$ correspond aux $d^*+1$ premières colonnes de $\boldsymbol X$, $d^*<d$, la contrainte consiste à reproduire des tendances polynomiales de degré $d^*$.
Cela introduit du biais mais réduit la variance. 
Le filtre de Musgrave se retrouve en modélisant $y_t$ linéaire ($d=1$) et $v$ préserve les constantes ($d^*=0$) et en prenant le filtre d'Henderson comme filtre symétrique. 
C'est-à-dire que l'on a $\boldsymbol U=\transp{\begin{pmatrix}1&\cdots&1\end{pmatrix}}$, $\boldsymbol Z=\transp{\begin{pmatrix}-h&\cdots&+h\end{pmatrix}}$, $\boldsymbol \delta=\delta_1$, $\boldsymbol D=\sigma^2\boldsymbol I$.
Ce filtre dépend du rapport $\lvert\delta_1/\sigma\rvert$ (modélisant le biais), qui est à fixer par l'utilisateur. 
En supposant que la tendance est linéaire et le biais constant, ce rapport est lié à l'I-C ratio $R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}$ (et l'on a $\delta_1/\sigma=2/(R\sqrt{\pi})$), qui est notamment utilisé dans X-11 pour déterminer la longueur de la moyenne mobile de Henderson à utiliser.
Pour des données mensuelles :

- Si le rapport est élevé ($3,5< R$), un filtre à 23 termes est utilisé (pour éliminer plus de bruit) et le rapport $R=4,5$ est utilisé dans X-11 pour définir le filtre de Musgrave.

- Si le rapport est faible ($R<1$), un filtre à 9 termes est utilisé et le rapport $R=1$ est utilisé dans X-11 pour définir le filtre de Musgrave.

- Dans le cas contraire (la plupart des cas), un filtre à 13 termes est utilisé et le rapport $R=3,5$ est utilisé dans X-11 pour définir le filtre de Musgrave.


TODO : parler des paramètres

### Filtres cascade

Les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.
Par ailleurs, comme notamment montré par @dagum1996new, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsqu'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 % des cycles de 9 ou 10 mois (généralement associés à du bruit plutôt qu'à la tendance-cycle).
Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois.
Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement.
Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier). 
C'est ainsi que le *Nonlinear Dagum Filter* (NLDF) a été développé et consiste à :

1. appliquer l'algorithme de correction des points atypiques de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ;
    
2. effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict et appliquer ensuite le filtre symétrique de 13 termes. 
    En supposant une distribution normale cela revient à modifier 48 % des valeurs de l'irrégulier.   

Les *cascade linear filter* [CLF, @clf], correspondent à une approximation des NLDF en utilisant un filtre de 13 termes et lorsque les prévisions sont obtenus à partir d'un modèle ARIMA(0,1,1) où $\theta=0,40.$
    
C'est cette moyenne mobile qui est utilisée par Statistique Canada pour l'estimation de la tendance-cycle^[
Voir par exemple <https://www.statcan.gc.ca/fr/quo/bdd/tendance-cycle>.
].
Pour l'estimation en temps réel, Statistique Canada utilise la méthode « couper-et-normaliser » qui consiste à recalculer les poids, à partir de la moyenne mobile symétrique, en n'utilisant que les poids associées aux observations disponibles (couper) et en renormalisant pour que la somme des coefficients soit égale à 1 (normaliser).
C'est l'approche qui sera utilisé dans cet article.


## Construction de moyennes mobiles robustes


En ajoutant un régresseur $O_t$ on peut donc obtenir une nouvelle moyenne mobile donc l'estimation de $\beta_0$ prendrait en compte l'effet modélisé par $x_t$
$$
y_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i} + \gamma O_{t+j}+\varepsilon_{t+j}
$$

Deux cas sont ici étudiés :

- Points atypiques additifs (AO) : un choc soudain à une certaine date puis un retour à la normal.
Si le choc est à la date $t_0$ le régresseur utilisé est alors $x_t=1_{t=t_0}$.

- Rupture en niveau (LS) : un changement de niveau à une certaine date.
Si le choc est à la date $t_0$ le régresseur utilisé est alors $x_t=-1_{t< t_0}$ pour l'estimation de $TC_t$ avec $t\leq t_0$ et $x_t=1_{t\geq t_0}$ sinon.


## Construction d'intervalles de confiance pour des moyennes mobiles

Soit $y_1,\dots,y_n$ une série chronologique observée.
On suppose qu'elle peut être décomposée en 
$$
y_t=\mu_t+\varepsilon_t,
$$
où $\mu_t$ est une composante déterministe à estimer et $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ est le bruit.

Soit $\theta = \begin{pmatrix}\theta_{-p},\dots,\theta_f\end{pmatrix}$ une moyenne mobile permettant d'estimer une composante inobservable $\mu_t$ (dans notre cas la tendance-cycle $TC_t$) à partir de $y_t$.
Cette estimation est donnée par $\hat \mu_t = \sum_{i=-p}^{+f}\theta_iy_{t+i}.$

Un intervalle de confiance de $\mathbb E[\hat{\mu}_t]$ peut être calculé à partir de la formule :
$$
I_t=\left[\hat{\mu}_t - q_{\alpha}\sqrt{\hat \sigma^2}\sqrt{\sum_{i=-p}^{+f}\theta_i^2};
 \hat{\mu}_t  + q_{\alpha}\sqrt{\hat \sigma^2}\sqrt{\sum_{i=-p}^{+f}\theta_i^2}\right]
$$
où 
$$
\hat\sigma^2=\frac{1}{n-p-f}\sum_{t=p+1}^{n-f}\frac{(y_t-\widehat{\mu}_t)^2}{1-2\theta_0^2+\sum_{i=-p}^{+f} \theta_i^2}.
$$
et $q_\alpha$ est le quantile d'ordre $\alpha$ d'une certaine loi de Student.
C'est un intervalle de confiance de $\mu_t$ lorsque l'on a un estimateur sans biais de $\mu_t$ ($\mathbb E[\hat{\mu}_t]=\sum_{i=-p}^{+f}\theta_iy_{t+i}=\mu_t$), ce qui n'est généralement pas le cas mais il est négligeablelorsque la fenêtre $p+f+1$ est petite.

Ces formules ces retrouvent par analogie avec la régression polynomiale locale.
En reprenant les notations de @Loader1999 pour la régression polynomiale locale et en adaptant à l'utilisation de moyennes mobiles, la variance $\hat\sigma^2$ peut être estimée par la somme des carrés des résidus normalisés :
$$
\hat\sigma^2=\frac{1}{(n-p-f)-2\nu_1+\nu_2}\sum_{t=p+1}^{n-f}(y_t-\widehat{\mu}_t)^2.
$$
$n-p-f$ termes sont utilisés car avec la moyenne mobile $\theta$ seulement $n-p-f$ observations peuvent être utilisées pour estimer $\sigma^2$.
$\nu_1$ et $\nu_2$ sont deux définitions de degrés de liberté d'une estimation locale (généralisation du nombre de paramètres d'un modèle paramétrique).
Notons $\boldsymbol H$ la *matrice chapeau* de taille $n\times n$ permettant de faire correspondre les données aux valeurs estimées :
$$
\begin{pmatrix}
\hat{\mu}_1\\ \vdots \\ \hat{\mu}_n
\end{pmatrix} = \boldsymbol H \boldsymbol Y
\text{ avec }
\boldsymbol Y = \begin{pmatrix}
y_1\\ \vdots \\ y_n
\end{pmatrix}.
$$
En considérant par convention que $\hat\mu_1=\dots=\hat{\mu}_p=\hat\mu_{n-f+1}=\dots=\hat\mu_n=0$ (puisque l'on ne peut pas estimer ces quantités avec la moyenne mobile $\boldsymbol \theta$), on a donc :
$$
\boldsymbol H=\begin{pmatrix}
&&\boldsymbol0_{p\times n} \\
\theta_{-p} & \cdots & \theta_f  & 0 & \cdots\\
0 & \theta_{-p} & \cdots & \theta_f  & 0 & \cdots
\\ & \ddots &&&\ddots\\
0 &\cdots&0& \theta_{-p} & \cdots & \theta_f \\
&&\boldsymbol0_{f\times n} 
\end{pmatrix},
$$
où $\boldsymbol 0_{p\times n}$ est la matrice de taille $p\times n$ ne contenant que de zéros.
On a :
$$
\begin{cases}
\nu_1 =\tr (L) = (n-p-f)\theta_0\\
\nu_2 = \tr (\transp{L}L) = (n-p-f) \sum_{i=-p}^{+f} \theta_i^2
\end{cases}.
$$
Si les bruits $\varepsilon_t$ sont indépendants et de variance $\sigma^2$, alors :
$$
\V{Y_t-\hat{\mu}_t}=\sigma^2 - 
2\underbrace{\cov{Y_t}{\hat{\mu}_t}}_{=\theta_0\sigma^2} + 
\underbrace{\V{\hat{\mu}_t}}_{=\sigma^2\sum_{i=-p}^{+f} \theta_i^2}
$$
et l'on a donc :
$$
\E{\hat\sigma^2}=\sigma^2 + \frac{1}{(n-p-f)-2\nu_1+\nu_2}\sum_{t=p+1}^{n-f}(\E{\hat \mu_t}-\mu_t)^2.
$$
L'estimateur $\hat\sigma^2$ est donc sans biais si $\hat\mu_t$ l'est aussi.


La somme des carrés des résidus peut s'écrire sous forme quadratique :
$$
\sum_{t=p+1}^{n-f}(y_t-\widehat{\mu}_t)^2 =
\transp{\boldsymbol Y}\boldsymbol \Gamma\boldsymbol Y 
$$
où :
$$
\boldsymbol \Gamma = \transp{(\boldsymbol I - \boldsymbol H)}(\boldsymbol I - \boldsymbol H\boldsymbol)\text{ avec }
\boldsymbol I = \begin{pmatrix}
&\boldsymbol0_{p\times n} \\
\boldsymbol 0_{(n-p-f)\times p} &I_{n-p-f} & \boldsymbol 0_{(n-p-f)\times f}  \\
&\boldsymbol0_{f\times n} 
\end{pmatrix}
$$
et $\boldsymbol I_{n-p-f}$ la matrice identité de taille $n-p-f.$
On a donc :
$$
\hat\sigma^2=\frac{1}{\tr{\boldsymbol \Gamma}}\transp{\boldsymbol Y}\boldsymbol \Gamma\boldsymbol Y
$$
et 
$$
\begin{cases}
\E{\hat\sigma^2}=\sigma^2\tr(\boldsymbol \Gamma)\\
\V{\hat\sigma^2}=2\sigma^4\frac{\tr(\boldsymbol \Gamma^2)}{\tr(\boldsymbol \Gamma)^2}
\end{cases}.
$$
En notant $\nu = \tr(\boldsymbol \Gamma)^2 / \tr(\boldsymbol \Gamma^2)$, on a donc :
$$
\begin{cases}
\E{\nu\frac{\hat\sigma^2}{\sigma^2}}=\nu\\
\V{\nu\frac{\hat\sigma^2}{\sigma^2}}=2\nu
\end{cases}.
$$
Les deux premiers moments de $\nu \hat\sigma^2/\sigma^2$ sont donc identiques à ceux d'un loi du chi-deux avec $\nu$ degrés de liberté : on peut donc approximer la distribution de $\hat\sigma^2$ par une loi du chi-deux.

Puisque $\V{\mu_t}=\sigma^2\sum_{i=-p}^{+f} \theta_i^2$, on a donc :
$$
\frac{\hat\mu_t-\E{\mu_t}}{\sqrt{\V{\mu_t}}}\sim\mathcal{T}(\nu)
$$
Le paramètre $\nu$ peut être calculé analytiquement en reconstruisant la matrice $\boldsymbol \Gamma$ (c'est ce qui est fait par défaut dans `rjd3filters::confint_filter()`).
Il peut aussi être approximé par $\tr(\boldsymbol \Gamma)$ (`rjd3filters::confint_filter(exact_df = FALSE)`) ce qui permet de réduire le temps de calcul^[
Sur une série mensuelle de 19 ans, le temps de calcul est d'environ 0,1 secondes et est divisé d'en moyenne 4 000 en utilisant l'approximation.
].

## Points atypiques étudiés

# Résultats

## Additive outlier (A0)

### Données simulées

#### Tendance de degré 0

```{r}
res <- readRDS(file.path("..", "results", "simul", "simul_ao_td0.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```


```{r}
#| label: fig-simul_ao-out-td0-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-simul_ao-out-td0-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3) 
```


```{r}
#| label: fig-simul_ao-out-td0-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```


#### Tendance de degré 1


```{r}
res <- readRDS(file.path("..", "results", "simul", "simul_ao_td1.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```


```{r}
#| label: fig-simul_ao-out-td1-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-simul_ao-out-td1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3) 
```


```{r}
#| label: fig-simul_ao-out-td1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

### IPI voitures

#### Août 1998


```{r}
res <- readRDS(file.path("..", "results", "AO", "ipi_voitures98.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est <- lapply(graph_est,function(p){
	p$CLF <- NULL
	p
})
graph_y <- plot_y(res)
graph_ci <- lapply(
	res$out, plot_confint,
	data = res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```


```{r}
#| label: fig-ipi_voitures98-out1-y
#| fig.cap: "Série brute"
graph_y[[1]]
```

```{r}
#| label: fig-ipi_voitures98-out1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est[[1]], ncol = 3) 
```


```{r}
#| label: fig-ipi_voitures98-out1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci[[1]], ncol = 3)
```

#### Août 1999


```{r}
#| label: fig-ipi_voitures98-out2-y
#| fig.cap: "Série brute"
graph_y[[2]]
```

```{r}
#| label: fig-ipi_voitures98-out2-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est[[2]], ncol = 3)
```


```{r}
#| label: fig-ipi_voitures98-out2-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci[[2]], ncol = 3)
```


#### 2004


```{r}
res <- readRDS(file.path("..", "results", "AO", "ipi_voitures2004.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```

```{r}
#| label: fig-ipi_voitures2004-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-ipi_voitures2004-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ipi_voitures2004-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

## Level shift (LS)

### Données simulées

#### Tendance de degré 0
```{r}
res <- readRDS(file.path("..", "results", "simul", "simul_ls_td0.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ls"]])
```


```{r}
#| label: fig-simul_ls-out-td0-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-simul_ls-out-td0-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3) 
```


```{r}
#| label: fig-simul_ls-out-td0-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

#### Tendance de degré 1
```{r}
res <- readRDS(file.path("..", "results", "simul", "simul_ls_td1.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ls"]])
```


```{r}
#| label: fig-simul_ls-out-td1-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-simul_ls-out-td1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3) 
```


```{r}
#| label: fig-simul_ls-out-td1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```



### IPI petrole brut

```{r}
res <- readRDS(file.path("..", "results", "LS", "ipi_petrole_brut10.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ls"]])
```

```{r}
#| label: fig-ipi_petrole_brut10-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-ipi_petrole_brut10-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ipi_petrole_brut10-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```



## Deux LS

### Retailx

```{r}
res <- readRDS(file.path("..", "results", "LSLS", "retailx2008.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["lsls"]])
```

```{r}
#| label: fig-retailx2008-y
#| fig.cap: "Série brute"
graph_y
```

```{r}
#| label: fig-retailx2008-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-retailx2008-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```


## AO puis LS

### Immatriculation de véhicules neufs

```{r}
res <- readRDS(file.path("..", "results", "AOLS", "immat2018.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["aols"]])
```

```{r}
#| label: fig-imat2018-y
#| fig.cap: "Série brute"
graph_y
```



```{r}
#| label: fig-imat2018-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
#| 
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-imat2018-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

## Point de retournement

### Retailx

```{r}
res <- readRDS(file.path("..", "results", "AO", "retailx2007.rds"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```

```{r}
#| label: fig-retailx2007-y
#| fig.cap: "Série brute"
graph_y
```



```{r}
#| label: fig-retailx2007-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-retailx2007-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

### ce160v

```{r}
res <- readRDS(file.path("..", "results", "AO", "ce160v2001.rds"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = robust_ff[["ao"]])
```

```{r}
#| label: fig-ce160v2001-y
#| fig.cap: "Série brute"
graph_y
```



```{r}
#| label: fig-ce160v2001-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 8
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ce160v2001-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave"
#| fig-width: 9
patchwork::wrap_plots(graph_ci, ncol = 3)
```

# Annexe



## Moyennes mobiles utilisées


```{r}
#| label: fig-mm-musgrave
#| fig.cap: "Henderson et filtre de musgrave"
ggdemetra3::ggplot_coef(lp_filter(), q= 0:6)
```

```{r}
#| label: fig-clf
#| fig.cap: "Cascade linear filter"
ggdemetra3::ggplot_coef(CLF, q= 0:6)
```

```{r}
#| label: fig-clfnn
#| fig.cap: "Cascade linear filter et cut and normalize"
ggdemetra3::ggplot_coef(CLF_CN, q= 0:6)
```

```{r}
#| label: fig-robust-ao
#| fig.cap: "Filtre robuste AO présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$ao$t0, q= 0:7)
```

```{r}
#| label: fig-robust-aols
#| fig.cap: "Filtre robuste AO puis LS présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$aols$`t-1`, q= 0:7)
```

```{r}
#| label: fig-robust-ls
#| fig.cap: "Filtre robuste LS présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$ls$t0, q= 0:7)
```

```{r}
#| label: fig-robust-lsls
#| fig.cap: "Filtre robuste deux LS présents aux deux dernières dates"
ggdemetra3::ggplot_coef(robust_ff$lsls$`t-1`, q= 0:7)
```
