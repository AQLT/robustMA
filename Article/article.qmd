---
title: "Estimation de la tendance-cycle avec des méthodes robustes aux points atypiques"
format: 
  html:
    fig-width: 9
    toc-depth: 3
    include-before-body: template/preambule.html
    output-file: index
  pdf:
    include-in-header: template/preambule.tex
    keep-tex: true
    classoption: french
    cite-method: biblatex
    pdf-engine: pdflatex
    papersize: A4
    fig-width: 7.5
    fig-height: 3.5
    toc: false # to include abstract first
    fig-pos: 'H'
crossref:
  tbl-prefix: table
  fig-prefix: figure
  sec-prefix: section
  eq-prefix: équation
execute: 
  cache: true
lang: fr
echo: false
toc: true
number-sections: true
tbl-cap-location: top
fig-cap-location: top
bibliography: biblio.bib
# csl: chicago-author-date.csl
---

```{r}
#| include: false
#| label: setup
library(rjd3filters)
library(ggplot2)
library(patchwork)
source("../0-functions.R", local = knitr::knit_global())
source("../0-functions-plot.R", local = knitr::knit_global())
lc_f <- lp_filter()
robust_ff <- readRDS("../data/robust_ff.rds")
y_as_plot <- FALSE
add_y <- FALSE
apply_consistent_y_lims <- function(this_plot){
    num_plots <- length(this_plot)
    y_lims <- lapply(this_plot, function(p) ggplot_build(p)$layout$panel_scales_y[[1]]$range$range)
    x_lims <- lapply(this_plot, function(p) ggplot_build(this_plot[[1]])$layout$panel_params[[1]]$x.range)
    min_y <- min(unlist(y_lims))
    max_y <- max(unlist(y_lims))
    for(i in seq_along(this_plot)) {
    	this_plot[[i]] <- this_plot[[i]] + coord_cartesian(ylim = c(min_y, max_y), xlim = NULL#x_lims[[i]]
    													   )
    }
    this_plot
}
```


# Introduction

L'analyse du cycle économique, et en particulier la détection rapide des points de retournement d'une série, est un sujet de première importance dans l'analyse de la conjoncture économique. 
Pour cela, les indicateurs économiques sont généralement corrigés des variations saisonnières.
Toutefois, afin d'améliorer leur lisibilité, il peut être nécessaire d'effectuer un lissage supplémentaire afin de réduire le bruit, et ainsi analyser la composante tendance-cycle.
Par construction, les méthodes d'extraction de tendance-cycle sont étroitement liées aux méthodes de désaisonnalisation.
En effet, afin d'estimer la composante saisonnière, les algorithmes de désaisonnalisation estiment préalablement une composante tendance-cycle.
Ainsi, même si les méthodes d'extraction de tendance-cycle sont généralement appliquées sur des séries corrigées des variations saisonnières, l'estimation de ces séries dépend également également des méthodes d'estimation de la tendance-cycle.

Les moyennes mobiles, ou les filtres linéaires, sont omniprésents dans les méthodes d'extraction du cycle économique et d'ajustement saisonnier^[
Une moyenne mobile est une méthode statistique qui consiste à appliquer une moyenne pondérée glissante à une série chronologique : à chaque date $t$ on calcule une moyenne pondérée de $p$ points passés et $q$ points futurs où $p,q\geq0$ dépend de la moyenne mobile.
]. 
Ainsi, la méthode de désaisonnalisation X-13ARIMA-SEATS utilise des moyennes mobiles de Henderson et des moyennes mobiles composites pour estimer les principales composantes d'une série chronologique. 
Au centre de la série, des filtres symétriques sont appliqués. 
Pour l'extraction de la tendance-cycle, le filtre symétrique le plus connu est celui de @henderson1916note, notamment utilisé dans l'algorithme de désaisonnalisation X-13ARIMA. 

Toutefois, ces moyennes mobiles, comme tout opérateur linéaire, sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.

D'autre part, les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.

<!-- Par ailleurs, comme notamment montré par @dagum1996new, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsqu'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 % des cycles de 9 ou 10 mois (généralement associés à du bruit plutôt qu'à la tendance-cycle). -->
<!-- Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois. -->
<!-- Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement. -->
<!-- Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier).  -->
<!-- C'est ainsi que le *Nonlinear Dagum Filter* (NLDF) a été développé et consiste à : -->

<!-- a. appliquer l'algorithme de correction des points atypiques de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ; -->

<!-- b. effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict et appliquer ensuite le filtre symétrique de 13 termes.  -->
<!-- En supposant une distribution normale cela revient à modifier 48 % des valeurs de l'irrégulier.    -->

<!-- Les *cascade linear filter* (CLF), notamment étudiés dans @dagumBianconcini2023, correspondent à une approximation des NLDF en utilisant un filtre de 13 termes et lorsque les prévisions sont obtenus à partir d'un modèle ARIMA(0,1,1) où $\theta=0,40.$ -->

<!-- Une piste d'étude serait alors d'étudier plus précisément l'effet des points atypiques sur l'estimation de la tendance-cycle et la détection des points de retournement, mais aussi d'explorer de nouveaux types de filtres asymétriques fondés sur des méthodes robustes (comme les régressions locales robustes, les médianes mobiles, etc.).  -->

<!-- En revanche, pour les estimations en temps réel, en raison de l'absence d'observations futures, toutes ces méthodes doivent s'appuyer sur des filtres asymétriques pour estimer les points les plus récents.  -->
<!-- Par exemple, même si X-13ARIMA-SEATS applique des moyennes symétriques aux prévisions obtenues à partir d'un modèle ARIMA, cela revient à appliquer des filtres asymétriques en fin de série, car les valeurs prédites sont des combinaisons linéaires de valeurs passées.  -->

<!-- Si ces moyennes mobiles asymétriques ont de bonnes propriétés concernant la taille des révisions futures induites par le processus de lissage^[Voir par exemple @pierce1980SA.], elles induisent également, par construction, des déphasages qui retardent en général la détection en temps réel des points de retournement. -->


# Méthodologie

L'hypothèse de base utilisée dans les méthodes de décomposition des séries temporelles est que la série chronologique observée, $y_t$, peut être décomposée en une composante de signal $\mu_t$ et une composante erratique $\varepsilon_t$ (appelée composante irrégulière) :
$$
f(y_t)=\mu_t+\varepsilon_t
$$
où $f$ désigne une transformation appropriée (généralement logarithmique ou aucune transformation).
Pour simplifier les notations ultérieures, $y_t$ désignera la série observée transformée.
La composante de bruit $\varepsilon_t$ est généralement supposée être un bruit blanc.
En supposant que la série chronologique initiale est désaisonnalisée (ou sans saisonnalité), le signal $\mu_t$ représente la tendance (variations sur une longue période) et cycle (mouvements cycliques superposés à la tendance à long terme), estimés ici conjointement et appelé tendance-cycle $TC_t$.
Autour d'un voisinage $h$ de $t$, cette composante peut être approximée localement par un polynôme de degré $d$ :
$$
TC_{t+i} = \sum_{j=0}^d\beta_j{i}^j+\xi_{t+i}\quad\forall i\in\{-h,-h+1,\dots,t+h\}
$$
avec $\xi_t$ un processus stochastique non corrélé avec $\varepsilon_t$.
Même si certains articles modélisent $\xi_t$ et $\varepsilon_t$ séparément [voir par exemple @GrayThomson2002], une hypothèse habituelle, utilisée dans cet article, est de rassembler $\xi_t$ et $\varepsilon_t$^[
Cela revient à supposer que le biais d'approximation de la tendance-cycle par un polynôme local est nul.
].
Ainsi, la tendance-cycle $TC_t$ est considérée comme déterministe et modélisée comme une tendance polynomiale de degré $d$.
Les coefficients $(\beta_0,\dots,\beta_d)$ peuvent être estimés par la méthode des moindres carrés pondérés.
L'estimation $\hat \beta_0$ fournit l'estimation du cycle de tendance $\widehat{TC}_t$ et on peut montrer que cela équivaut à appliquer une moyenne mobile.
Cette moyenne mobile est symétrique ($\widehat{TC}_t$ est estimée en utilisant autant d'observation avant et après $t$) mais pour l'estimation des derniers points (lorsque l'on ne peut pas utiliser autant d'observations avant et après $t$) il est nécessaire de s'appuyer sur des moyennes mobiles ad-hoc qui sont asymétriques.

L'estimation de $\widehat{TC}_t$ étant faite par un méthode linéaire, elle est sensible à la présence de points atypiques qui peuvent notamment entraîner des révisions importantes.
C'est par exemple le cas pendant la crise du COVID-19 qui a conduit Statistiques Canada et Australian Bureau of Statistics à suspendre la publication des tendances-cycles pendant cette période.
Comme notamment discuté par @matthewssapw4, différentes stratégies peuvent être adoptées pendant les périodes de fortes "turbulences" (comme la crise du COVID-19) où les points atypiques sont importants et donc les estimations directes :

- Ne pas publier les estimations pendant cette période : c'est ce qui est fait par l'Australian Bureau of Statistics qui ne publie pas d'estimation tendance-cycle entre avril 2020 et mars 2022.

- Estimer les points atypiques avec un modèle RegARIMA, estimer la tendance-cycle sur la série corrigée et "réintroduire" les effets corrigés sur la tendance-cycle. 
L'inconvénient de cette approche est que l'estimation du point atypique dépend du modèle ARIMA utilisé et de la période d'estimation.

- Estimer la tendance-cycle sur l'ensemble de la période et remplacement des données estimées après la rupture par celle estimée en commençant la série après la rupture et après la rupture (série segmentée d'un côté).
L'inconvénient de cette approche est que les estimations avant la rupture vont être biaisées par la présence de la rupture et que celles après la rupture reposent sur des moyennes mobiles asymétriques (avec un biais plus important que les moyennes mobiles symétriques utilisées pour les estimations finales).

- Diviser la série en deux au niveau de la rupture et estimer la tendance-cycle sur chaque segment (série segmentée des deux côtés).
L'inconvénient de cette approche est que les estimations avant la rupture et après la rupture reposent sur des moyennes mobiles asymétriques (avec un biais plus important que les moyennes mobiles symétriques utilisées pour les estimations finales).
C'est l'approche privilégiée par Statistique Canada.

L'objectif de cet article est d'étudier des approches alternatives à celles de @matthewssapw4.
D'une part en étudiant les estimations en temps réel de méthodes robustes à la présence de points (@sec-methodes-robustes) ;
d'autre part en proposant une méthode de construction de moyennes mobiles linéaires robustes et en les comparant aux estimations des moyennes mobiles classiques (@sec-methodes-lineaires).


## Méthodes robustes {#sec-methodes-robustes}

Dans cette étude nous étudions six méthodes robustes d'estimation locale de la moyenne implémentées dans la fonction `robfilter::robreg.filter()` [@robfilter].
Dans ce package, la tendance-cycle est supposée être localement linéaire :
$$
y_{t+i}=\underbrace{\beta_{0,t}+\beta_{1,t}i}_{TC_{t+i}}+\varepsilon_{t,i}
$$
Et l'on note :
$$
r_{t+i}=y_{t+i}-(\beta_{0,t}+\beta_{1,t}i).
$$
On a donc :
$$
\widehat{TC}_t = \hat\beta_{0,t}.
$$

Les différentes méthodes étudiées sont :

- Médiane mobile (MED) :

$$
\widehat{TC}_t =
\underset{i=-h,\dots,h}{\med}y_{t+i}
$$

- Régression médiane répétée --- *Repeated Median* (RM) [@rm]
$$
\hat{\beta}_{1,t}=
\underset{i=-h,\dots,h}{\med}
\left\{ \underset{i\ne j}{\med}\frac{y_{t+i}-y_{t+j}}{i-j} \right\}
$$
et 
$$
\widehat{TC}_t=
\underset{i=-h,\dots,h}{\med}
\left\{ y_{t+i}-i\hat{\beta}_{1,t} \right\}.
$$

- Régression des moindres carrés médians --- *Least Median of Squares* (LMS) [@lms] :
$$
(\widehat{TC}_t,\hat\beta_{1,t})=
\underset{\beta_{0,t},\beta_{1,t}}{\argmin}
\left\{ \underset{i=-h,\dots,h}{\med}r^2_{t+i} \right\}
$$

- Régression des moindre carrés élagués --- *Least Trimmed Squares* (LTS) [@lts] :
$$
(\widehat{TC}_t,\hat\beta_{1,t})=
\underset{\beta_{0,t},\beta_{1,t}}{\argmin}
\left\{ \sum_{i=-h}^h r^2_{t+i} \right\}
$$


- Régression des moindres quartiles différenciés --- *Least Quartile Difference* (LQD) [@lqd].
En notant $r_{i,j} = r_i-r_j$
$$
\underset{\beta_{1,t}}{\argmin}
Q_{2h+1}(r_{t-h},\dots,r_{t+h})
$$
avec
$$
Q_{2h+1}(r_{t-h},\dots,r_{t+h}) = \left\{
|r_i-r_j|;i<j
\right\}_{\binom{h_p}{2}:\binom{2h+1}{2}}
$$
le quantile d'ordre $\binom{h_p}{2}$ sur les $\binom{2h+1}{2}$ éléments de l'ensemble $\{|r_i-r_j|;i<j\}$ et $h_p=[(2h+1+p+1)/2]$ avec $p=1$ le nombre de régresseurs (hors constante).
La fonction objectif ne dépendant pas de la constante, elle est estimée a posteriori par exemple en utilisant la formule :
$$
\widehat{TC}_t =
\underset{i=-h,\dots,h}{\med}y_{t+i}-\hat \beta_{1,t}i.
$$

- Régression profonde --- *Deepest Regression* (DR) [@DeepestRegression]
$$
(\widehat{TC}_t,\hat\beta_{1,t})=\underset{\tilde\beta_{0},\tilde\beta_{1}}{\argmax}\left\{ rdepth(\tilde\beta_{0},\tilde\beta_{1}) \right\}
$$
où la profondeur de la regression --- *regression depth* ($rdepth$) --- d'un ajustement $(\tilde\beta_{0},\tilde\beta_{1})$ est définie comme :
$$
rdepth(\tilde\beta_{0},\tilde\beta_{1}) = \underset{-h\leq i\leq h}{\min}
\left\{ \min \left\{ L^+(i) + R^-(i);R^+(i) + L^-(i) \right\}\right\}
$$
avec :
$$
\begin{cases}
L^+(i) = L^+_{\tilde\beta_{0},\tilde\beta_{1}}(i) = cardinal \left\{ j\in\{-h,\dots,i\} : r_j(\tilde\beta_{0},\tilde\beta_{1})\leq 0 \right\} \\
R^-(i) = R^-_{\tilde\beta_{0},\tilde\beta_{1}}(i) = cardinal \left\{ j\in\{i+1,\dots,h\} : r_j(\tilde\beta_{0},\tilde\beta_{1})< 0 \right\}
\end{cases}
$$
et $L^-(i)$ et $R^+(i)$ définis de manière analogue.

Pour l'estimation en temps réel, le package `robfilter` utilise les mêmes algorithmes mais en n'utilisant que les données disponibles (donc en utilisant plus de points avant $t$ qu'après $t$).

Dans le cadre de l'analyse conjoncturelle, il n'est généralement pas plausible de modéliser, autour des points de retournement, une tendance de degré 1.
Il n'est toutefois pas possible de changer cela dans le package `robfilter` et il n'existe pas d'autre package permettant d'appliquer simplement ces méthodes^[
Les méthodes LTS et LMS sont implémentées dans le package `MASS` [@MASS], il est nécessaire de réimplémenter le processus d'estimation locale.
Nous n'avons pas trouvé d'autres fonctions permettant d'utiliser les autres méthodes mais elles pourraient être implémentées facilement.
].
Même si cette modélisation n'est pas toujours optimale, elle reste plausible dans la majorité des cas et la facilité d'utilisation de `robfilter` font de ces méthodes un bon point de comparaison aux moyennes mobiles linéaires classiquement utilisées pour l'estimation de la tendance-cycle et présentées dans la section suivante.

## Moyennes mobiles linéaires {#sec-methodes-lineaires}

### Moyenne mobile de Henderson et de Musgrave {#sec-lp}
Les moyennes mobiles classiques peuvent être obtenues par analogie avec la régression polynomiale locale.
En reprenant les notations de @proietti2008, on suppose que notre série chronologique $y_t$ peut être décomposée en :
$$
y_t=TC_t+\varepsilon_t,
$$
où $TC_t$ est la tendance-cycle et $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ est le bruit. 
La tendance-cycle $TC_t$ est localement approchée par un polynôme de degré $d$, de sorte que dans un voisinage $h$ de $t$ on a $TC_t\simeq m_{t}$ avec :
$$
\forall j\in\left\{-h,-h+1,\dots,h\right\},\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i,t}j^{i}.
$$
En notation matricielle :
$$
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{\boldsymbol y_t}=\underbrace{\begin{pmatrix}1 & -h & h^{2} & \cdots & (-h)^{d}\\
1 & -(h-1) & (h-1)^{2} & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & h-1 & (h-1)^{2} & \cdots & (h-1)^{d}\\
1 & h & h^{2} & \cdots & h^{d}
\end{pmatrix}}_{\boldsymbol X}\underbrace{\begin{pmatrix}\beta_{0,t}\\
\beta_{1,t}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d,t}
\end{pmatrix}}_{\boldsymbol \beta_t}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\boldsymbol \varepsilon_t}.
$$ {#eq-lpp}

L'estimation des paramètres $\boldsymbol \beta_t$ peut être obtenue moindres carrés pondérés --- *weighted least squares* (WLS) --- à partir d'un ensemble de poids $(\kappa_j)_{-h\leq j \leq h}$ appelés noyaux.
En notant $\boldsymbol K=diag(\kappa_{-h},\dots,\kappa_{h})$ il vient $\hat{\boldsymbol\beta_t}=(\transp{\boldsymbol X}\boldsymbol K\boldsymbol X)^{-1}\transp{\boldsymbol X}\boldsymbol K\boldsymbol y_t.$
Avec $\boldsymbol e_1=\transp{\begin{pmatrix}1 &0 &\cdots&0 \end{pmatrix}}$, l'estimation de la tendance-cycle est :
$$
\widehat{TC}_t=\widehat{\beta}_{0,t}=\boldsymbol e_{1}\hat{\boldsymbol \beta_t}=\transp{\boldsymbol \theta}\boldsymbol y_t=\sum_{j=-h}^{h}\theta_{j}y_{t-j}\text{ avec }\boldsymbol \theta=\boldsymbol K\boldsymbol X(\transp{\boldsymbol X}\boldsymbol K\boldsymbol X)^{-1}\boldsymbol e_{1}.
$$ {#eq-mmsym}
En somme, l'estimation de la tendance $\hat{m}_{t}$ est obtenue en appliquant une moyenne mobile symétrique $\boldsymbol \theta$ à $y_t$.

On retrouve la moyenne mobile de Henderson avec $d=2$ (ou $d=3$) et en utilisant les noyaux :
$$
\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right].
$$
<!-- De plus, puisque $\transp{\boldsymbol X}\boldsymbol \theta=\boldsymbol e_{1}$ on a: -->
<!-- $$ -->
<!-- \sum_{j=-h}^{h}\theta_{j}=1,\quad\forall r\in\left\{1,2,\dots,d\right\}:\sum_{j=-h}^{h}j^{r}\theta_{j}=0. -->
<!-- $$ -->
<!-- La moyenne mobile $\boldsymbol \theta$ conserve donc les tendances polynomiales de degré $d$. -->

Pour le cas asymétrique, @proietti2008 proposent une méthode générale pour construire les filtres asymétriques qui permet de faire un compromis biais-variance.
Il s'agit d'une généralisation des filtres asymétriques de @musgrave1964set (utilisés dans l'algorithme de désaisonnalisation X-13ARIMA).
En réecrivant l' @eq-lpp :
$$
\boldsymbol y_t=\begin{pmatrix}\boldsymbol U &\boldsymbol Z\end{pmatrix}
\begin{pmatrix}\boldsymbol \gamma_t \\ \boldsymbol \delta_t\end{pmatrix}+
\boldsymbol \varepsilon_t,\quad
\boldsymbol \varepsilon_t\sim\mathcal{N}(0,\boldsymbol D).
$$ {#eq-lpgeneralmodel}
où $[\boldsymbol U,\boldsymbol Z]$ est de rang plein et forme un sous-ensemble des colonnes de $\boldsymbol X$.
L'objectif est de trouver une moyenne mobile asymétrique $\boldsymbol \theta^{(a)}$ qui minimise l'erreur quadratique moyenne de révision (à la moyenne mobile symétrique $\boldsymbol \theta$) sous certaines contraintes.
Ces contraintes sont représentées par la matrice $\boldsymbol U=\transp{\begin{pmatrix}\transp{\boldsymbol U_{p}}&\transp{\boldsymbol U_{f}}\end{pmatrix}}$ : $\transp{\boldsymbol U_p}\boldsymbol \theta^{(a)}=\transp{\boldsymbol U}\boldsymbol \theta$ (avec $\boldsymbol U_p$ la matrice $(h+q+1)\times (d+1)$ qui contient les observations de la matrice $\boldsymbol U$ connues lors de l'estimation par le filtre asymétrique).
C'est ce qui est implémenté dans la fonction `rjd3filters::mmsre_filter()`.

Lorsque $\boldsymbol U$ correspond aux $d^*+1$ premières colonnes de $\boldsymbol X$, $d^*<d$, la contrainte consiste à reproduire des tendances polynomiales de degré $d^*$.
Cela introduit du biais mais réduit la variance. 
Le filtre de Musgrave se retrouve en modélisant $y_t$ linéaire ($d=1$) et $\theta^{(a)}$ préserve les constantes ($d^*=0$) et en prenant le filtre d'Henderson comme filtre symétrique. 
C'est-à-dire que l'on a $\boldsymbol U=\transp{\begin{pmatrix}1&\cdots&1\end{pmatrix}}$, $\boldsymbol Z=\transp{\begin{pmatrix}-h&\cdots&+h\end{pmatrix}}$, $\boldsymbol \delta_t=\delta_{1,t}$, $\boldsymbol D=\sigma^2\boldsymbol I$.
Ce filtre dépend du rapport $\lvert\delta_{1,t}/\sigma\rvert$ (modélisant le biais), qui est à fixer par l'utilisateur. 
En supposant que la tendance est linéaire et le biais constant ($\delta_{1,t}=\delta_1$), ce rapport est lié à l'I-C ratio $R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}$ (et l'on a $\delta_1/\sigma=2/(R\sqrt{\pi})$), qui est notamment utilisé dans X-11 pour déterminer la longueur de la moyenne mobile de Henderson à utiliser.
Pour des données mensuelles :

- Si le rapport est élevé ($3,5< R$), un filtre symétrique à 23 termes est utilisé (pour éliminer plus de bruit) et le rapport $R=4,5$ est utilisé dans X-11 pour définir le filtre de Musgrave.

- Si le rapport est faible ($R<1$), un filtre symétrique à 9 termes est utilisé et le rapport $R=1$ est utilisé dans X-11 pour définir le filtre de Musgrave.

- Dans le cas contraire (la plupart des cas), un filtre symétrique à 13 termes est utilisé et le rapport $R=3,5$ est utilisé dans X-11 pour définir le filtre de Musgrave.

Dans cet article, seules des séries mensuelles sont étudiés et le filtre de Henderson à 13 termes sera utilisé comme filtre symétrique.
Par simplification et par cohérence avec ce qui est utilisé dans X-11, le ratio $\delta_1/\sigma$ sera fixé de sorte à avoir $R=3,5$.
C'est également ce qui est fait par l'Australian Bureau of Statistics [@abs2003].
Toutefois, comme notamment montré dans @jos2024AQLT, une paramétrisation locale de ce paramètre pourrait être préférée.

### Filtres cascade

Les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.
Par ailleurs, comme notamment montré par @dagum1996new, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsqu'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 % des cycles de 9 ou 10 mois (généralement associés à du bruit plutôt qu'à la tendance-cycle).
Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois.
Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement.
Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier). 
C'est ainsi que le *Nonlinear Dagum Filter* (NLDF) a été développé et consiste à :

1. appliquer l'algorithme de correction des points atypiques de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ;
    
2. effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict et appliquer ensuite le filtre symétrique de 13 termes. 
    En supposant une distribution normale cela revient à modifier 48 % des valeurs de l'irrégulier.   

Les *cascade linear filter* [CLF, @clf], correspondent à une approximation des NLDF en utilisant un filtre de 13 termes et lorsque les prévisions sont obtenus à partir d'un modèle ARIMA(0,1,1) où $\theta=0,40.$
    
C'est cette moyenne mobile qui est utilisée par Statistique Canada pour l'estimation de la tendance-cycle^[
Voir par exemple <https://www.statcan.gc.ca/fr/quo/bdd/tendance-cycle>.
].
Pour l'estimation en temps réel, Statistique Canada utilise la méthode « couper-et-normaliser » qui consiste à recalculer les poids, à partir de la moyenne mobile symétrique, en n'utilisant que les poids associées aux observations disponibles (couper) et en renormalisant pour que la somme des coefficients soit égale à 1 (normaliser).
C'est l'approche qui sera utilisé dans cet article.


### Construction de moyennes mobiles robustes

Une autre façon de construire des moyennes mobiles robustes aux points atypiques est d'exploiter l'approche polynomiale locale présentée dans la @sec-lp.
Pour la construire la moyenne mobile utilisée pour l'estimation finale de la tendance-cycle, il suffit pour cela d'ajouter dans l'@eq-lpp un ou plusieurs régresseurs $\boldsymbol O_t$ pour prendre en compte l'effet du point atypique modélisé :
$$
y_{t+j}=\sum_{i=0}^{d}\beta_{i,t}j^{i} + \boldsymbol O_{t+j} \boldsymbol\zeta_t+\varepsilon_{t+j}.
$$

On a alors :
$$
\widehat{TC}_t=\transp{\boldsymbol \theta_t^{(r)}}\boldsymbol y_t=\sum_{j=-h}^{h}\theta_{j,t}^{(r)}y_{t-j}
$$
avec
$$
\boldsymbol \theta_t^{(r)}=\boldsymbol K\begin{pmatrix}\boldsymbol X &  \mathcal O_{t}\end{pmatrix}(\transp{\begin{pmatrix}\boldsymbol X &  \mathcal O_{t}\end{pmatrix}}\boldsymbol K \begin{pmatrix}\boldsymbol X &  \mathcal O_{t}\end{pmatrix})^{-1}\boldsymbol e_{1}
\text{ et }
\mathcal O_{t} = \begin{pmatrix}
\boldsymbol O_{t-h} \\ \vdots\\\boldsymbol O_{t+h}
\end{pmatrix}.
$$

La moyenne mobile $\boldsymbol \theta_t^{(r)}$ est centrée (autant de points avant et après $t$ sont utilisées pour estimer $\widehat{TC}_t$) mais n'est plus forcément symétrique (pour au moins un $i\in\{1,\dots,h\}$ on a : $\theta_{-i,t}^{(r)}\ne\theta_{i,t}^{(r)}$).

Le filtre de Henderson peut être obtenu par approximation locale d'un polynôme de degré 2.
Puisqu'il est symétrique, il préserve également les tendances de degré 3 et peut également être obtenu par approximation locale d'un polynôme de degré 3.
En revanche, même si la moyenne mobile $\boldsymbol \theta_t^{(r)}$ est centrée (autant de points avant et après $t$ sont utilisées pour estimer $\widehat{TC}_t$), elle n'est plus forcément symétrique (pour au moins un $i\in\{1,\dots,h\}$ on a : $\theta_{-i,t}^{(r)}\ne\theta_{i,t}^{(r)}$).
Pour construire une moyenne mobile robuste associée au filtre de Henderson, le choix entre modélisation d'un polynôme local de degré 2 ($d=2$) ou 3 ($d=3$) aura donc un impact sur la moyenne mobile finale.
Dans cet article nous utiliserons par convention la valeur $d=3$ afin de préserver les mêmes tendances que la moyenne mobile de Henderson.

<!-- Comme $\transp{\begin{pmatrix}\boldsymbol X &  \mathcal O_{t}\end{pmatrix}}\boldsymbol \theta_t^{(r)}=\boldsymbol e_{1}$ -->


Pour la construction des moyennes mobiles asymétriques, le même principe est appliqué à l'@eq-lpgeneralmodel en estimant sans biais $\boldsymbol\zeta_t$ :
$$
\boldsymbol y_t=\begin{pmatrix}\boldsymbol U & \mathcal O_{t}\end{pmatrix}\begin{pmatrix}\boldsymbol \gamma_t \\ \boldsymbol\zeta_t \end{pmatrix}+\boldsymbol Z\boldsymbol \delta_t+\boldsymbol \varepsilon_t,\quad
\boldsymbol \varepsilon_t\sim\mathcal{N}(\boldsymbol 0,\boldsymbol D).
$$
La moyenne mobile asymétrique $\boldsymbol \theta^{(a)(r)}_t$ est trouvée par minimisation  qui minimise l'erreur quadratique moyenne de révision (à la moyenne mobile $\boldsymbol \theta_t^{(r)}$) sous contraintes.
Ces contraintes sont représentées par la matrice
$\begin{pmatrix}\boldsymbol U & \mathcal O_{t}\end{pmatrix} = \begin{pmatrix}\boldsymbol U_{p} & \mathcal O_{p,t} \\\boldsymbol U_{f} & \mathcal O_{f,t} \end{pmatrix}$ : 
$$
\transp{\begin{pmatrix}\boldsymbol U_{p} & \mathcal O_{p,t} \end{pmatrix}}\boldsymbol \theta^{(a)}=\transp{\begin{pmatrix}\boldsymbol U & \mathcal O_{t}\end{pmatrix}}\boldsymbol \theta
$$ 

Par convention, lorsque $\mathcal O_{t}$ ou $\mathcal O_{p,t}$ est la matrice nulle, on garde la modélisation de la @sec-lp.

Dans cette étude, les deux types de points atypiques les plus couramment rencontrés dans l'analyse de données macroéconomiqués sont étudiés :

- Les points atypiques additifs (AO, *additive outlier*) : un choc ponctuelle à une date particulière puis un retour à la normal (grève, mesure exceptionnelle, erreur de mesure par exemple liée à de la non réponse, etc.).
Le choc affecte donc l'irrégulier et ne devrait pas avoir d'impact sur la tendance-cycle.
Si le choc le choc apparaît à la date $t_0$ il peut être modélisé par le régresseur $O_t=\1_{t=t_0}$ qui vaut $1$ à la date $t_0$ et $0$ sinon.

- Les ruptures en niveau (LS, *level shift*) : un changement soudain et durable du niveau moyen de la série (choc structurel, changement de politique économique, etc.).
Le choc affecte donc la tendance-cycle et ne devrait pas avoir d'irrégulier.
Si le choc est à la date $t_0$ il peut être modélisé par le régresseur $O_{t+j}=-\1_{t< t_0}$ si $t<t_0$ (l'estimation de $\beta_{0,t}$ ne prend pas en compte le choc en niveau) et $O_{t+j}=\1_{t\geq t_0}$ si $t\geq t_0$ (l'estimation de $\beta_{0,t}$ prend en compte le choc en niveau).


L'inconvénient de cette approche, par rapport aux méthodes robustes présentées dans la @sec-methodes-robustes, est que cela suppose de connaître la date et la nature du point atypique.
Une approche en deux temps peut être utilisée :

1. Partir sur un a priori basé sur une information économique (ex : baisse attendue de l'économie au moment des confinements durant le COVID-19, hausse attendue suite à une mise en place d'une politique...) ou sur des modèles statistiques (comme le module de détection des points atypiques de X-13ARIMA-SEATS basé sur un modèle RegARIMA).

2. Valider la modélisation retenue en comparant l'intervalle de confiance de l'estimation de la tendance-cycle avec les moyennes mobiles de Henderson et de Musgrave avec l'estimation en utilisant les moyennes mobiles robustes.
La @sec-ic décrit la méthodologie pour la construction d'intervalles de confiance pour des estimations basées sur des moyennes mobiles.

Toutefois, pour l'estimation en temps réel, sauf à avoir une information extérieure, il est difficile de distinguer un *level shift* (LS) d'un ou plusieurs *additive outlier* (AO) consécutifs.
Le choix du type de point atypique aura une influence forte sur l'estimation (puisque les LS affectent la tendance-cycle et les AO l'irrégulier) et passer d'une spécification à l'autre entraînera des révisions importantes.
Ces révisions seront par ailleurs plus importantes que si, avant de décider de la bonne modélisation à adopter, les premières estimations avaient été faites avec des moyennes moyennes mobiles classiques.



Dans la suite ces moyennes mobiles seront appelées moyenne mobile de Henderson ou de Musgrave robuste.

### Construction d'intervalles de confiance pour des moyennes mobiles {#sec-ic}

Soit $y_1,\dots,y_n$ une série chronologique observée.
On suppose qu'elle peut être décomposée en 
$$
y_t=\mu_t+\varepsilon_t,
$$
où $\mu_t$ est une composante déterministe à estimer et $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ est le bruit.

Soit $\boldsymbol\theta = \begin{pmatrix}\theta_{-p},\dots,\theta_f\end{pmatrix}$ une moyenne mobile permettant d'estimer une composante inobservable $\mu_t$ (dans notre cas la tendance-cycle $TC_t$) à partir de $y_t$.
Cette estimation est donnée par $\hat \mu_t = \sum_{i=-p}^{+f}\theta_iy_{t+i}.$

Un intervalle de confiance de $\mathbb E[\hat{\mu}_t]$ peut être calculé à partir de la formule :
$$
I_t=\left[\hat{\mu}_t - q_{\alpha}\sqrt{\hat \sigma^2}\sqrt{\sum_{i=-p}^{+f}\theta_i^2};
 \hat{\mu}_t  + q_{\alpha}\sqrt{\hat \sigma^2}\sqrt{\sum_{i=-p}^{+f}\theta_i^2}\right]
$$
où 
$$
\hat\sigma^2=\frac{1}{n-p-f}\sum_{t=p+1}^{n-f}\frac{(y_t-\widehat{\mu}_t)^2}{1-2\theta_0^2+\sum_{i=-p}^{+f} \theta_i^2}.
$$
et $q_\alpha$ est le quantile d'ordre $\alpha$ d'une certaine loi de Student.
C'est un intervalle de confiance de $\mu_t$ lorsque l'on a un estimateur sans biais de $\mu_t$ ($\mathbb E[\hat{\mu}_t]= \sum_{i=-p}^{+f}\theta_iy_{t+i}= \mu_t$), ce qui n'est généralement pas le cas, mais il est négligeable, lorsque la fenêtre $p+f+1$ est petite.

Ces formules ces retrouvent par analogie avec la régression polynomiale locale.
En reprenant les notations de @Loader1999 pour la régression polynomiale locale et en adaptant à l'utilisation de moyennes mobiles, la variance $\hat\sigma^2$ peut être estimée par la somme des carrés des résidus normalisés :
$$
\hat\sigma^2=\frac{1}{(n-p-f)-2\nu_1+\nu_2}\sum_{t=p+1}^{n-f}(y_t-\widehat{\mu}_t)^2.
$$
$n-p-f$ termes sont utilisés car avec la moyenne mobile $\theta$ seulement $n-p-f$ observations peuvent être utilisées pour estimer $\sigma^2$.
$\nu_1$ et $\nu_2$ sont deux définitions de degrés de liberté d'une estimation locale (généralisation du nombre de paramètres d'un modèle paramétrique).
Notons $\boldsymbol H$ la *matrice chapeau* de taille $n\times n$ permettant de faire correspondre les données aux valeurs estimées :
$$
\begin{pmatrix}
\hat{\mu}_1\\ \vdots \\ \hat{\mu}_n
\end{pmatrix} = \boldsymbol H \boldsymbol Y
\text{ avec }
\boldsymbol Y = \begin{pmatrix}
y_1\\ \vdots \\ y_n
\end{pmatrix}.
$$
En considérant par convention que $\hat\mu_1=\dots=\hat{\mu}_p=\hat\mu_{n-f+1}=\dots=\hat\mu_n=0$ (puisque l'on ne peut pas estimer ces quantités avec la moyenne mobile $\boldsymbol \theta$), on a donc :
$$
\boldsymbol H=\begin{pmatrix}
&&\boldsymbol0_{p\times n} \\
\theta_{-p} & \cdots & \theta_f  & 0 & \cdots\\
0 & \theta_{-p} & \cdots & \theta_f  & 0 & \cdots
\\ & \ddots &&&\ddots\\
0 &\cdots&0& \theta_{-p} & \cdots & \theta_f \\
&&\boldsymbol0_{f\times n} 
\end{pmatrix},
$$
où $\boldsymbol 0_{p\times n}$ est la matrice de taille $p\times n$ ne contenant que de zéros.
On a :
$$
\begin{cases}
\nu_1 =\tr (H) = (n-p-f)\theta_0\\
\nu_2 = \tr (\transp{H}H) = (n-p-f) \sum_{i=-p}^{+f} \theta_i^2
\end{cases}.
$$
Si les bruits $\varepsilon_t$ sont indépendants et de variance $\sigma^2$, alors :
$$
\V{Y_t-\hat{\mu}_t}=\sigma^2 - 
2\underbrace{\cov{Y_t}{\hat{\mu}_t}}_{=\theta_0\sigma^2} + 
\underbrace{\V{\hat{\mu}_t}}_{=\sigma^2\sum_{i=-p}^{+f} \theta_i^2}
$$
et l'on a donc :
$$
\E{\hat\sigma^2}=\sigma^2 + \frac{1}{(n-p-f)-2\nu_1+\nu_2}\sum_{t=p+1}^{n-f}(\E{\hat \mu_t}-\mu_t)^2.
$$
L'estimateur $\hat\sigma^2$ est donc sans biais si $\hat\mu_t$ l'est aussi.


La somme des carrés des résidus peut s'écrire sous forme quadratique :
$$
\sum_{t=p+1}^{n-f}(y_t-\widehat{\mu}_t)^2 =
\transp{\boldsymbol Y}\boldsymbol \Delta\boldsymbol Y 
$$
où :
$$
\boldsymbol \Delta = \transp{(\boldsymbol I - \boldsymbol H)}(\boldsymbol I - \boldsymbol H\boldsymbol)\text{ avec }
\boldsymbol I = \begin{pmatrix}
&\boldsymbol0_{p\times n} \\
\boldsymbol 0_{(n-p-f)\times p} &\boldsymbol I_{n-p-f} & \boldsymbol 0_{(n-p-f)\times f}  \\
&\boldsymbol0_{f\times n} 
\end{pmatrix}
$$
et $\boldsymbol I_{n-p-f}$ la matrice identité de taille $n-p-f.$
On a donc :
$$
\hat\sigma^2=\frac{1}{\tr{\boldsymbol \Delta}}\transp{\boldsymbol Y}\boldsymbol \Delta\boldsymbol Y.
$$
Si les bruits $\varepsilon_t$ sont normalement distribués indépendants et $\hat\sigma^2$, la distribution est :
$$
\transp{\boldsymbol Y}\boldsymbol \Delta\boldsymbol Y\overset{\mathcal L}{=}\sigma^2\sum_{j=1}^n\lambda_jZ_j,
$$
où les $\lambda_j$ sont les valeurs propres de $\boldsymbol\Delta$ et les $Z_j$ soit des lois indépendantes $\chi_1^2.$
Il vient :
$$
\begin{cases}
\E{\hat\sigma^2}=\sigma^2\frac{1}{\tr{\boldsymbol \Delta}}\underbrace{\sum_{j=1}^n\lambda_j}_{=\tr{\boldsymbol \Delta}}=\sigma^2\\
\V{\hat\sigma^2}=\sigma^4\frac{1}{\tr{(\boldsymbol \Delta)}^2}\sum_{j=1}^n2\lambda_j^2=2\sigma^4\frac{\tr(\boldsymbol \Delta^2)}{\tr(\boldsymbol \Delta)^2}
\end{cases}.
$$
En notant $\nu = \tr(\boldsymbol \Delta)^2 / \tr(\boldsymbol \Delta^2)$, on a donc :
$$
\begin{cases}
\E{\nu\frac{\hat\sigma^2}{\sigma^2}}=\nu\\
\V{\nu\frac{\hat\sigma^2}{\sigma^2}}=2\nu
\end{cases}.
$$
Les deux premiers moments de $\nu \hat\sigma^2/\sigma^2$ sont donc identiques à ceux d'un loi du chi-deux avec $\nu$ degrés de liberté.
La loi $\hat\sigma^2$ étant difficile à calculer dans le cadre de la régression locale, on donc approximer sa distribution de $\hat\sigma^2$ par une loi du chi-deux.

Puisque $\V{\mu_t}=\sigma^2\sum_{i=-p}^{+f} \theta_i^2$, on a donc :
$$
\frac{\hat\mu_t-\E{\mu_t}}{\sqrt{\V{\mu_t}}}\sim\mathcal{T}(\nu)
$$
Le numérateur de $\nu$ se calcule facilement puisque l'on a :
$$
\tr(\boldsymbol \Delta) = (n-p-f)\left(1-2\theta_0+\sum_{i=-p}^{+f} \theta_i^2\right).
$$
En revanche il est difficile d'avoir une formule simplifiée pour le dénominateur $\tr(\boldsymbol \Delta^2).$
Ce dernier peut être calculé en reconstruisant la matrice $\boldsymbol \Delta$ (produit de matrices de taille $n\times n$) ou par le produit d'une matrice de taille $1\times (p+f+1)$ et d'une matrice de taille $(p+f+1)\times(p+f+1)$ pour réduire le temps de calcul (voir @sec-df-var) :
$$
\tr((\boldsymbol \Delta\transp{\boldsymbol \Delta})^2)=(n-(p+f))L_0^2+2\sum_{k=1}^{p+f}(n-(p+f)-k)L_k^2
$$
où $L_0,\dots,L_{p+f}$ sont définis par :
$$
\begin{pmatrix}w_{-p} & \cdots & w_f\end{pmatrix}
\begin{pmatrix}
w_{-p} &0 & 0 &\cdots& 0 \\
w_{-p+1} & w_{-p} & 0 & \ddots & \vdots \\
w_{-p+2} & w_{-p+1}& w_{-p} & \ddots & \vdots\\
\vdots & \vdots & \vdots & \ddots &\vdots \\
w_f & w_{f-1} & w_{f-2}&\ddots&w_{-p}\\
\end{pmatrix}=\begin{pmatrix} L_0 & \cdots & L_{p+f} \end{pmatrix}
$$
avec $\boldsymbol w = \begin{pmatrix}w_{-p},\dots,w_f\end{pmatrix}$ la moyenne mobile telle que $w_0=1-\theta_0$ et $w_i=-\theta_i$ pour $i\ne0$.
C'est la méthode de calcul utilisée par défaut dans `rjd3filters`.
Le calcul de $\nu$ peut aussi être approximé par $\tr(\boldsymbol \Delta)$ (`rjd3filters::confint_filter(exact_df = FALSE)`) ce qui permet de réduire davantage le temps de calcul^[
Sur une série mensuelle de 19 ans ($n=228$), en utilisant la moyenne mobile de Henderson de 13 termes ($p=f=6$), le temps de calcul $\nu$ est d'en moyenne 0,11 seconde (sur 1 000 évaluations).
Il est divisé d'environ 350 en utilisant la formule proposée dans cet article (environ 0,31 milliseconde) et d'environ 5 800 en utilisant l'approximation $\nu\simeq \tr(\boldsymbol \Delta)$ (environ 0,02 milliseconde).
]. 

# Résultats

Les différentes méthodes sont tout d'abord comparées sur des séries simulées (choc de 10 %, tendance de degré 0, 1 ou 2, @sec-res-series-simul) puis sur des séries réelles (@sec-res-series-reelles).

## Séries simulées {#sec-res-series-simul}


```{r}
##########
### AO ###
##########
res_ao_td0 <- readRDS(file.path("..", "results", "simul", "simul_ao_td0.RDS"))
graph_est_ao_td0 <- get_all_plots(res_ao_td0, y_as_plot = y_as_plot, add_y = add_y)
graph_est_ao_td0$CLF <- NULL
graph_y_ao_td0 <- plot_y(res_ao_td0) + 
	ggplot2::ggtitle("AO - tendance degré 0")
graph_ci_ao_td0 <- plot_confint(
	res_ao_td0,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste AO" = robust_ff[["ao"]]
		)
	)
res_ao_td1 <- readRDS(file.path("..", "results", "simul", "simul_ao_td1.RDS"))
graph_est_ao_td1 <- get_all_plots(res_ao_td1, y_as_plot = y_as_plot, add_y = add_y)
graph_est_ao_td1$CLF <- NULL
graph_y_ao_td1 <- plot_y(res_ao_td1) + 
	ggplot2::ggtitle("AO - tendance degré 1")
graph_ci_ao_td1 <- plot_confint(
	res_ao_td1,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste AO" = robust_ff[["ao"]]
		)
	)
res_ao_td2 <- readRDS(file.path("..", "results", "simul", "simul_ao_td2.RDS"))
graph_est_ao_td2 <- get_all_plots(res_ao_td2, y_as_plot = y_as_plot, add_y = add_y)
graph_est_ao_td2$CLF <- NULL
graph_y_ao_td2 <- plot_y(res_ao_td2) + 
	ggplot2::ggtitle("AO - tendance degré 2")
graph_ci_ao_td2 <- plot_confint(
	res_ao_td2,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste AO" = robust_ff[["ao"]]
		)
	)

##########
### LS ###
##########

res_ls_td0 <- readRDS(file.path("..", "results", "simul", "simul_ls_td0.RDS"))
graph_est_ls_td0 <- get_all_plots(res_ls_td0, y_as_plot = y_as_plot, add_y = add_y)
graph_est_ls_td0$CLF <- NULL
graph_y_ls_td0 <- plot_y(res_ls_td0) + 
	ggplot2::ggtitle("LS - tendance degré 0")
graph_ci_ls_td0 <- plot_confint(
	res_ls_td0,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste LS" = robust_ff[["ls"]],
		"MM robuste AO-AO" = robust_ff[["aoao"]]
		)
	)
res_ls_td1 <- readRDS(file.path("..", "results", "simul", "simul_ls_td1.RDS"))
graph_est_ls_td1 <- get_all_plots(res_ls_td1, y_as_plot = y_as_plot, add_y = add_y)
graph_est_ls_td1$CLF <- NULL
graph_y_ls_td1 <- plot_y(res_ls_td1) + 
	ggplot2::ggtitle("LS - tendance degré 1")
graph_ci_ls_td1 <- plot_confint(
	res_ls_td1,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste LS" = robust_ff[["ls"]],
		"MM robuste AO-AO" = robust_ff[["aoao"]]
		)
	)
res_ls_td2 <- readRDS(file.path("..", "results", "simul", "simul_ls_td2.RDS"))
graph_est_ls_td2 <- get_all_plots(res_ls_td2, y_as_plot = y_as_plot, add_y = add_y)
graph_est_ls_td2$CLF <- NULL
graph_y_ls_td2 <- plot_y(res_ls_td2) + 
	ggplot2::ggtitle("LS - tendance degré 2")
graph_ci_ls_td2 <- plot_confint(
	res_ls_td2,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste LS" = robust_ff[["ls"]],
		"MM robuste AO-AO" = robust_ff[["aoao"]]
		)
	)
```


Afin d'illustrer les performances des différentes méthodes, les différentes estimations sont comparées sur des séries simulées dans un cas extrême : lorsque l'irrégulier est nul.
Trois tendances sont simulées, de degré 0, 1 ou 2, et un choc positif de 10 % est introduit en janvier 2022 : soit un choc ponctuel (*additive outlier*, affectant l'irrégulier) soit un choc permanent (*level shift*, affectant la tendance-cycle).
Les séries simulées commencent en janvier 2018, soit 4 ans avant le choc afin de limiter l'impact des points atypiques sur le calcul des intervalles de confiance (puisque l'irrégulier est nul, l'intervalle de confiance est également nul avant l'introduction du choc).
La @fig-simul-y montre les différentes séries simulées.


```{r}
#| label: fig-simul-y
#| fig.cap: "Séries simulées avec un choc ponctuel (*additive outlier*, AO) ou permanent (*level shift*, LS) en janvier 2022"
#| fig-height: 4
graph_y_ao_td0 + graph_y_ao_td1 + graph_y_ao_td2 +
	graph_y_ls_td0 + graph_y_ls_td1 + graph_y_ls_td2 +
	plot_layout(guides = 'collect', axes = "collect", nrow = 2) &
	theme(axis.text.x = ggplot2::element_text(size=8, angle=20,
											  vjust=1.1, hjust=1))
```


Pour les chocs ponctuels (*additive outlier*, AO), lorsque la tendance est de degré 0, aucune des méthodes robustes n'est influencée par la présence de l'AO, tout comme la moyenne mobile de Musgrave robuste (@fig-simul-ao-out-td0-est).
Les estimations avec les moyennes mobiles de Musgrave et CLF sont biaisées avec des révisions importantes à la date du choc, légèrement plus petites pour la méthode CLF.
L'analyse des intervalles de confiance valident clairement la présence du point atypique (@fig-simul-ao-out-td0-ci).\
Lorsque la tendance est de degré 1 (figures [-@fig-simul-ao-out-td1-est] et [-@fig-simul-ao-out-td1-ci]) les résultats sont similaires sauf pour la médiane mobile qui en temps réel conduit à des estimations peu réalistes.
Comme notamment indiqué par @gather2006online, cela suggère d'utiliser des quantiles plus élevés que la médiane pour les estimations en temps réel avec les méthodes robustes.
Un léger biais négatif s'observe pour la méthode de Musgrave robuste : cela provient du fait que pour la création de ces moyennes mobiles la pente est fixé (ratio $\delta_1/\sigma$ fixé et $\sigma$ a priori fixé) à une valeur différente de la pente simulée.
Cela suggère de préférer une paramétrisation locale de ce paramètre, comme proposé par @jos2024AQLT.\
Lorsque la tendance est de degré 2 (figures [-@fig-simul-ao-out-td2-est] et [-@fig-simul-ao-out-td2-ci]), les résultats avec les moyennes mobiles sont similaires (révisions importantes autour du choc pour Musgrave et CLF et faibles pour Musgrave robuste).
Pour les méthodes robustes, les estimations finale modélisent bien le retournement de tendance malgré le fait que juste une tendance de degré 1 est modélisée.
En revanche, contrairement aux moyennes mobiles de Musgrave (classiques et robustes) qui modélisent également en fin de période des tendances de degré 1, les estimations intermédiaires sont peu réalistes et conduisent à des révisions importantes.
Cela suggère que pour l'estimation en temps réel avec les méthodes robustes, il faudrait, autour des points de retournement, modéliser une tendance de degré 2 (ce qui n'est pas possible dans `robfilter`).


```{r}
#| label: fig-simul-ao-out-td0-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle pour une série simulée avec une tendance de degré 0 et un choc ponctuel (*additive outlier*, AO) en janvier 2022"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est_ao_td0, ncol = 3)
```


```{r}
#| label: fig-simul-ao-out-td0-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave et filtres de musgrave robuste pour une série simulée avec une tendance de degré 0 et un choc ponctuel (*additive outlier*, AO) en janvier 2022"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci_ao_td0, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```



```{r}
#| label: fig-simul-ao-out-td1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle pour une série simulée avec une tendance de degré 1 et un choc ponctuel (*additive outlier*, AO) en janvier 2022"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est_ao_td1, ncol = 3)
```


```{r}
#| label: fig-simul-ao-out-td1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de musgrave et filtres de musgrave robuste pour une série simulée avec une tendance de degré 1 et un choc ponctuel (*additive outlier*, AO) en janvier 2022"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci_ao_td1, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


```{r}
#| label: fig-simul-ao-out-td2-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle pour une série simulée avec une tendance de degré 2 et un choc ponctuel (*additive outlier*, AO) en janvier 2022"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est_ao_td2, ncol = 3)
```


```{r}
#| label: fig-simul-ao-out-td2-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave et filtres de Musgrave robuste pour une série simulée avec une tendance de degré 1 et un choc ponctuel (*additive outlier*, AO) en janvier 2022"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci_ao_td2, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


Pour les chocs permanents (*level shift*, LS), à l'exception de la régression profonde (DR) et de la régression médiane répétée (RM), les estimations finales de la tendance-cycle avec les méthodes robustes ne sont pas influencées par la présence du choc (figures [-@fig-simul-ls-out-td0-est],  [-@fig-simul-ls-out-td1-est] et [-@fig-simul-ls-out-td2-est]).
En revanche, aucune des estimations intermédiaires ne prend en compte le changement de niveau à la bonne date (il faut attendre l'estimation de juin 2022 pour que le changement de niveau soit pris en compte). 
Les méthodes sont en quelque sorte "trop robustes" et cela suggère encore d'utiliser des quantiles plus élevés que la médiane pour les estimations en temps réel.\
Les moyennes mobiles linéaires classiques (Musgrave et CLF) donnent des résultats similaires : la rupture en niveau est lissée, les points avant le choc sont donc sur-estimés et ceux après le choc sous-estimés.
Il y a également d'importantes révisions pour la première estimation de janvier 2022 (date du choc).
Il n'y a en revanche quasiment aucune révision pour les moyennes mobile de Musgrave robuste à un choc permanent en janvier 2022.\
Les intervalles de confiance des estimations de la moyenne mobile de Musgrave sont comparés à la moyenne mobile de Musgrave robuste au choc permanent et à la moyenne mobile de Musgrave robuste à un choc ponctuel en janvier et février 2022  (figures [-@fig-simul-ls-out-td0-ci],  [-@fig-simul-ls-out-td1-ci] et [-@fig-simul-ls-out-td2-ci]).
Cela permet de simuler le cas où on ne sait pas, en temps réel, trancher sur la nature du choc : soit un choc permanent, soit un choc ponctuel (lorsque l'on se place en janvier 2022), soit deux chocs ponctuels (lorsque l'on se place en février 2022).
En mars 2022, l'observation de 3 chocs de même ampleur confirme qu'il s'agit d'un choc permanent et non ponctuel.
Si l'on s'est trompé dans la nature du choc, jusqu'en février 2022 les révisions sont nulles pour les estimations avant le choc mais importantes pour celles après (janvier et février, puisque le choc est affecté à l'irrégulier et non pas à la tendance-cycle).

```{r}
#| label: fig-simul-ls-out-td0-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle pour une série simulée avec une tendance de degré 0 et un choc permanent (*level shift*, LS) en janvier 2022"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est_ls_td0, ncol = 3)
```


```{r}
#| label: fig-simul-ls-out-td0-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave et filtres de Musgrave robuste pour une série simulée avec une tendance de degré 0 et un choc permanent (*level shift*, LS) en janvier 2022"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci_ls_td0, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


```{r}
#| label: fig-simul-ls-out-td1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle pour une série simulée avec une tendance de degré 1 et un choc permanent (*level shift*, LS) en janvier 2022"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est_ls_td1, ncol = 3)
```


```{r}
#| label: fig-simul-ls-out-td1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave et filtres de Musgrave robuste pour une série simulée avec une tendance de degré 1 et un choc permanent (*level shift*, LS) en janvier 2022"
apply_consistent_y_lims(patchwork::wrap_plots(graph_ci_ls_td1, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


```{r}
#| label: fig-simul-ls-out-td2-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle pour une série simulée avec une tendance de degré 2 et un choc permanent (*level shift*, LS) en janvier 2022"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est_ls_td2, ncol = 3)
```


```{r}
#| label: fig-simul-ls-out-td2-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave et filtres de Musgrave robuste pour une série simulée avec une tendance de degré 2 et un choc permanent (*level shift*, LS) en janvier 2022"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci_ls_td2, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


## Séries réelles {#sec-res-series-reelles}

## Deux LS

### Retailx

```{r}
res <- readRDS(file.path("..", "results", "LSLS", "retailx2008.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res, n_xlabel = 6)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = list(
		"MM robuste LS-LS" = robust_ff[["lsls"]],
		"MM robuste AO-AO" = robust_ff[["aoao"]]
		),
	show.legend = TRUE
)
```

```{r}
#| label: fig-retailx2008-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y
```

```{r}
#| label: fig-retailx2008-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-retailx2008-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


## AO puis LS

### Immatriculation de véhicules neufs

```{r}
res <- readRDS(file.path("..", "results", "AOLS", "immat2018.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res, n_xlabel = 6)
# graph_ci <- plot_confint(
# 	res,
# 	default_filter = lc_f,
# 	robust_f = robust_ff[["aols"]])
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = list("Robust AO-LS" = robust_ff[["aols"]],
					"Robust AO" = robust_ff[["ao"]]),
	show.legend = TRUE
)
```

```{r}
#| label: fig-imat2018-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y
```



```{r}
#| label: fig-imat2018-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est, ncol = 3)
```

```{r}
#| label: fig-imat2018-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```

## Point de retournement

### Retailx

```{r}
res <- readRDS(file.path("..", "results", "AO", "retailx2007.rds"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res, n_xlabel = 6)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = list("MM robuste AO" = robust_ff[["ao"]]))
```

```{r}
#| label: fig-retailx2007-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y
```



```{r}
#| label: fig-retailx2007-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-retailx2007-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```

### ce160v

```{r}
res <- readRDS(file.path("..", "results", "AO", "ce160v2001.rds"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res, n_xlabel = 6)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = list("MM robuste AO" = robust_ff[["ao"]]))
```

```{r}
#| label: fig-ce160v2001-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y
```



```{r}
#| label: fig-ce160v2001-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ce160v2001-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```

\newpage
\appendix

# Annexes


## Moyennes mobiles utilisées


```{r}
#| label: fig-mm-musgrave
#| fig.cap: "Henderson et filtre de Musgrave"
ggdemetra3::ggplot_coef(lp_filter(), q= 0:6)
```

```{r}
#| label: fig-clf
#| fig.cap: "Cascade linear filter"
ggdemetra3::ggplot_coef(CLF, q= 0:6)
```

```{r}
#| label: fig-clfnn
#| fig.cap: "Cascade linear filter et cut and normalize"
ggdemetra3::ggplot_coef(CLF_CN, q= 0:6)
```

```{r}
#| label: fig-robust-ao
#| fig.cap: "Filtres robustes à la présence d'un *additive outlier* (AO) à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$ao$t0, q= 0:7)
```

```{r}
#| label: fig-robust-aols
#| fig.cap: "Filtre robuste AO puis LS présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$aols$`t-1`, q= 0:7)
```

```{r}
#| label: fig-robust-ls
#| fig.cap: "Filtre robuste LS présent à la dernière date"
ggdemetra3::ggplot_coef(robust_ff$ls$t0, q= 0:7)
```

```{r}
#| label: fig-robust-lsls
#| fig.cap: "Filtre robuste deux LS présents aux deux dernières dates"
ggdemetra3::ggplot_coef(robust_ff$lsls$`t-1`, q= 0:7)
```

## Calcul du degré de liberté de la variance {#sec-df-var}

Soient $\boldsymbol\theta = \begin{pmatrix}\theta_{-p},\dots,\theta_f\end{pmatrix}$ et $\boldsymbol \Delta = \transp{(\boldsymbol I - \boldsymbol H)}(\boldsymbol I - \boldsymbol H\boldsymbol)$ avec :
$$
\boldsymbol H=\begin{pmatrix}
&&\boldsymbol0_{p\times n} \\
\theta_{-p} & \cdots & \theta_f  & 0 & \cdots\\
0 & \theta_{-p} & \cdots & \theta_f  & 0 & \cdots
\\ & \ddots &&&\ddots\\
0 &\cdots&0& \theta_{-p} & \cdots & \theta_f \\
&&\boldsymbol0_{f\times n}
\end{pmatrix}\text{ et }
\boldsymbol I = \begin{pmatrix}
&\boldsymbol0_{p\times n} \\
\boldsymbol 0_{(n-p-f)\times p} &\boldsymbol I_{n-p-f} & \boldsymbol 0_{(n-p-f)\times f}  \\
&\boldsymbol0_{f\times n}
\end{pmatrix},
$$
où $\boldsymbol 0_{p\times n}$ est la matrice de taille $p\times n$ ne contenant que de zéros et $\boldsymbol I_{n-p-f}$ la matrice identité de taille $n-p-f.$

L'objectif de cette annexe est de montrer que le calcul de $\tr(\boldsymbol \Delta^2)$ peut se simplifier par le produit d'une matrice de taille $1\times (p+f+1)$ et d'une matrice de taille $(p+f+1)\times(p+f+1)$, ce qui permet de réduire le temps de calcul du degré de liberté de la loi de Student utilisée pour la construction d'intervalles de confiance :
$$
\tr((\boldsymbol \Delta\transp{\boldsymbol \Delta})^2)=(n-(p+f))L_0^2+2\sum_{k=1}^{p+f}(n-(p+f)-k)L_k^2
$$
où $L_0,\dots,L_{p+f}$ sont définis par :
$$
\begin{pmatrix}w_{-p} & \cdots & w_f\end{pmatrix}
\begin{pmatrix}
w_{-p} &0 & 0 &\cdots& 0 \\
w_{-p+1} & w_{-p} & 0 & \ddots & \vdots \\
w_{-p+2} & w_{-p+1}& w_{-p} & \ddots & \vdots\\
\vdots & \vdots & \vdots & \ddots &\vdots \\
w_f & w_{f-1} & w_{f-2}&\ddots&w_{-p}\\
\end{pmatrix}=\begin{pmatrix} L_0 & \cdots & L_{p+f} \end{pmatrix}
$$
avec $\boldsymbol w = \begin{pmatrix}w_{-p},\dots,w_f\end{pmatrix}$ la moyenne mobile telle que $w_i=\1_{i=0}-\theta_i$.


Soient $i,j\in\{1,\dots,n\},$ posons par convention $w_i=0$ pour $i\notin[-p,f].$
On a ${\boldsymbol \Delta}_{i,j}=w_{i-j}\1_{i\in[p,n-f]}$ et :
\begin{align*}
({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})_{i,j} &=\sum_{k=1}^n{\boldsymbol \Delta}_{i,k}{\boldsymbol \Delta}_{j,k}=\sum_{k=1}^nw_{i-k}w_{j-k} \1_{(i,j)\in[p,n-f]^2}\\
&=\sum_{k=-p}^fw_{k}w_{i-j-k}\1_{(i,j)\in[p,n-f]^2}.
\end{align*}
L'ensemble des termes de $({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})_{i,j}$ se calcule par le produit matriciel :
$$
\begin{pmatrix}w_{-p} & \cdots & w_f\end{pmatrix}
\begin{pmatrix}
w_{-p} &0 & 0 &\cdots& 0 \\
w_{-p+1} & w_{-p} & 0 & \ddots & \vdots \\
w_{-p+2} & w_{-p+1}& w_{-p} & \ddots & \vdots\\
\vdots & \vdots & \vdots & \ddots &\vdots \\
w_f & w_{f-1} & w_{f-2}&\ddots&w_p\\
\end{pmatrix}=\begin{pmatrix} L_0 & \cdots & L_{p+f} \end{pmatrix}.
$$
Puis :
\begin{align*}
(({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})^2)_{i,i} &= \sum_{j=1}^n({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})_{i,j}({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})_{j,i}\\
&=\sum_{j=1}^n({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})_{i,j}^2\text{ car la matrice }{\boldsymbol \Delta}\transp{{\boldsymbol \Delta}}\text{ est symétrique} \\
&=\sum_{j=1}^n\left(\sum_{k=-p}^fw_{k}w_{i-j-k}\1_{(i,j)\in[p,n-f]^2}\right)^2\\
&=\sum_{j=p}^{n-f}\left(\sum_{k=-p}^fw_{k}w_{i-j-k}\right)^2\1_{i\in[p,n-f]}.
\end{align*}
Donc $(({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})^2)_{i,i} = 0$ si $i\notin \in[p,n-f].$
Pour $i\in[p,n-f],$ comme pour tout $k\in [-p,f]$ on a $w_{i-j-k} =0$ si $j<i-(f+p)$ ou si $j>i-(f+p)$, il vient :
\begin{align*}
(({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})^2)_{i,i} &= \sum_{j=\max(i-(f+p),1)}^{\min(i+(f+p),n)}\left(\sum_{k=-p}^fw_{k}w_{i-j-k}\right)^2 \\
&=\sum_{j=-(f+p)}^{f+p}\left(\sum_{k=-p}^fw_{k}w_{k-j}\right)^2\1_{j\in [1-i,n-i]} \\
&=\sum_{j=-(f+p)}^{f+p}\left(L_{|j|}\right)^2\1_{j\in [1-i,n-i]}\text{ par symétrie autour de }0.
\end{align*}
En somme :
\begin{align*}
\tr(({\boldsymbol \Delta}\transp{{\boldsymbol \Delta}})^2)& = \sum_{i=p}^{n-f} \sum_{j=-(f+p)}^{f+p}\left(L_{|j|}\right)^2\1_{j\in [1-i,n-i]} \\
&= (n-(p+f))L_0^2+2(n-(p+f)-1)L_1^2 + 2(n-(p+f)-2)L_2^2 +\dots+2(n-2(p+f))L_{p+f}^2 \\
&=(n-(p+f))L_0^2+2\sum_{k=1}^{p+f}(n-(p+f)-k)L_k^2.
\end{align*}

## Autres exemples

### IPI voitures

#### Août 1998


```{r}
res <- readRDS(file.path("..", "results", "AO", "ipi_voitures98.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est <- lapply(graph_est,function(p){
	p$CLF <- NULL
	p
})
graph_y <- plot_y(res, n_xlabel = 6)
graph_ci <- lapply(
	res$out, plot_confint,
	data = res,
	default_filter = lc_f,
	robust_f = list("MM robuste AO" = robust_ff[["ao"]]))
```


```{r}
#| label: fig-ipi-voitures98-out1-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y[[1]]
```

```{r}
#| label: fig-ipi-voitures98-out1-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est[[1]], ncol = 3)
```


```{r}
#| label: fig-ipi-voitures98-out1-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

patchwork::wrap_plots(graph_ci[[1]], ncol = 3)
```

#### Août 1999


```{r}
#| label: fig-ipi-voitures98-out2-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y[[2]]
```

```{r}
#| label: fig-ipi-voitures98-out2-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est[[2]], ncol = 3)
```


```{r}
#| label: fig-ipi-voitures98-out2-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

patchwork::wrap_plots(graph_ci[[2]], ncol = 3)
```


#### 2004


```{r}
res <- readRDS(file.path("..", "results", "AO", "ipi_voitures2004.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res, n_xlabel = 6)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = list("MM robuste AO" = robust_ff[["ao"]]))
```

```{r}
#| label: fig-ipi-voitures2004-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y
```

```{r}
#| label: fig-ipi-voitures2004-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ipi-voitures2004-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```

## Level shift (LS)


### IPI petrole brut

```{r}
res <- readRDS(file.path("..", "results", "LS", "ipi_petrole_brut10.RDS"))
graph_est <- get_all_plots(res, y_as_plot = y_as_plot, add_y = add_y)
graph_est$CLF <- NULL
graph_y <- plot_y(res, n_xlabel = 6)
graph_ci <- plot_confint(
	res,
	default_filter = lc_f,
	robust_f = list("MM robuste LS" = robust_ff[["ls"]]))
```

```{r}
#| label: fig-ipi-petrole-brut10-y
#| fig.cap: "Série brute"
#| fig-height: 2
graph_y
```

```{r}
#| label: fig-ipi-petrole-brut10-est
#| fig.cap: "Estimations en temps réel de la tendance-cycle"
#| fig-height: 6
#|
patchwork::wrap_plots(graph_est, ncol = 3)
```


```{r}
#| label: fig-ipi-petrole-brut10-ci
#| fig.cap: "Intervalles de confiance pour les filtres de Musgrave"

apply_consistent_y_lims(patchwork::wrap_plots(graph_ci, ncol = 3)) + 
	plot_layout(guides = 'collect', axes = "collect") &
	theme(legend.position='bottom')
```


